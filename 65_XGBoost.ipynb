{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd821353-99cd-48ae-8d80-da0e99f0c43a",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Ensemble methods</font>\n",
    "<p> <font color=\"Yellow\" size=\"5\"><b>4_XGBoost</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bb341-c5e9-4f92-9422-b18d827f67d7",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is a popular, highly efficient, and scalable implementation of the Gradient Boosting framework. It has become one of the most widely used algorithms in machine learning competitions due to its speed, performance, and scalability. XGBoost is designed to optimize both accuracy and computational efficiency and includes regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47019ba-2ff2-45cd-b83b-96617d19da5c",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Key Features of XGBoost:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Gradient Boosting:</font> Like traditional gradient boosting, XGBoost builds an ensemble of weak learners (usually decision trees) in a sequential manner, where each new model focuses on the residuals (errors) of the previous model.</li>\n",
    "     <li><font color=\"orange\">Regularization:</font> XGBoost incorporates both L1 (Lasso) and L2 (Ridge) regularization to reduce overfitting and improve model generalization.</li>\n",
    "     <li><font color=\"orange\">Handling Missing Data:</font> XGBoost can handle missing data directly during training by automatically learning the best direction to split missing values.</li>\n",
    "     <li><font color=\"orange\">Parallelization:</font> XGBoost is optimized for performance and supports parallel and distributed computing for faster training.</li>\n",
    "     <li><font color=\"orange\">Tree Pruning:</font> XGBoost uses a more sophisticated approach to pruning decision trees, which helps to build deeper trees when necessary, enhancing model performance.</li>\n",
    "     <li><font color=\"orange\">Custom Objective Functions and Evaluation Metrics:</font> XGBoost allows users to define custom objective functions and evaluation metrics for specific use cases.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35b0e55-a756-4134-a5c5-2380071d2f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9815\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       0.95      1.00      0.98        21\n",
      "           2       1.00      0.93      0.96        14\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  0  0]\n",
      " [ 0 21  0]\n",
      " [ 0  1 13]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Create the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display the classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af647700-b3d9-4cc4-b127-1b10af065650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6895dc8-6755-4856-a3a0-eb362895406e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
