{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c6c3f4-b673-48ca-a10a-6a2f1d074f47",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\"><b>Nearest Neighbor Methods</font>\n",
    "<p><font color=\"Yellow\" size=\"5\"><b>2_ADASYN (Adaptive Synthetic Sampling)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fdb8f-f0ba-4625-80d4-4076fcfe7827",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>ADASYN (Adaptive Synthetic Sampling Approach for Imbalanced Learning)</font>\n",
    "\n",
    "ADASYN is an over-sampling technique that focuses on the generation of synthetic samples for the minority class by adaptively selecting the samples that are harder to learn, i.e., those that are closer to the decision boundary. This is different from traditional techniques like SMOTE, where synthetic data is generated uniformly across the minority class. ADASYN aims to improve the learning of the classifier by generating more synthetic samples near the border where the minority class is underrepresented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52355a36-cba6-431a-ad9e-c2a2025accae",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>How ADASYN Works:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Identify Hard-to-Learn Samples:</font>\n",
    "        The algorithm first computes the k-nearest neighbors (k-NN) for each minority class sample.\n",
    "        The samples that are more difficult to classify (i.e., those with fewer majority neighbors) are given a higher weight and are over-sampled more.</li>\n",
    "    <li><font color=\"orange\">Generate Synthetic Samples:</font>\n",
    "        Synthetic samples are generated using the SMOTE technique, but the number of synthetic samples is proportional to the difficulty of the minority class samples. Harder-to-learn samples are over-sampled more, leading to a more balanced dataset that also focuses on improving the boundary between classes.</li>\n",
    "    <li><font color=\"orange\">Balanced Dataset:</font>\n",
    "        ADASYN results in a more balanced dataset, especially focusing on the regions where the classifier would have the most difficulty in classifying the minority class.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9869dd6-65f4-433b-b586-29e656e22768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before ADASYN: Counter({0: 898, 1: 102})\n",
      "Class distribution after ADASYN: Counter({0: 898, 1: 890})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       270\n",
      "           1       0.95      0.95      0.95       267\n",
      "\n",
      "    accuracy                           0.95       537\n",
      "   macro avg       0.95      0.95      0.95       537\n",
      "weighted avg       0.95      0.95      0.95       537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying ADASYN\n",
    "print(\"Class distribution before ADASYN:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply ADASYN to balance the dataset\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the class distribution after applying ADASYN\n",
    "print(\"Class distribution after ADASYN:\", Counter(y_resampled))\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (RandomForest) on the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a6129-7b9f-436a-bc1f-a0b9edcaf48d",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Advantages of ADASYN:\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Focus on Hard Samples:</font> Unlike SMOTE, ADASYN generates more synthetic samples in regions where the classifier struggles most, making it more effective for some problems.</li>\n",
    "    <li><font color=\"orange\">Improves Boundary Learning:</font> By focusing on the decision boundary between classes, ADASYN improves the model's ability to distinguish between the minority and majority classes.</li>\n",
    "    <li><font color=\"orange\">Dynamic Sampling:</font> ADASYN dynamically adjusts the number of synthetic samples generated for each minority class sample based on its difficulty.</li></ol>\n",
    "\n",
    "<font color=\"pink\" size=4>Limitations of ADASYN:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Computationally Expensive:</font> It is more computationally expensive than SMOTE because it computes the k-nearest neighbors for each sample.</li>\n",
    "    <li><font color=\"orange\">Overfitting Risk:</font> By focusing heavily on the minority class and its boundary, there is a risk of overfitting, especially if the minority class has a lot of noise or if there are too many synthetic samples.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f81e5-fdeb-4ac9-89db-f18ffa9c2133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
