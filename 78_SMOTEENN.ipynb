{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810ee928-b5a5-44df-a809-8cfb2bd3313d",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Hybrid Model</font>\n",
    "<p><font color=\"Yellow\" size=\"5\"><b>1_SMOTEENN (SMOTE + Edited Nearest Neighbors)</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c47348-faf2-40e6-9c85-2edaa3d1b6e3",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>SMOTEENN is a hybrid technique that combines two methods to address class imbalance in datasets:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">SMOTE (Synthetic Minority Over-sampling Technique):</font> This technique creates synthetic samples for the minority class by interpolating between existing minority class samples.</li>\n",
    "    <li><font color=\"orange\">ENN (Edited Nearest Neighbors):</font> This is an under-sampling technique that removes noisy samples and borderline samples from both classes by looking at their nearest neighbors. It cleans up the dataset by removing ambiguous samples that could confuse a classifier.</li></ol>\n",
    "<p>\n",
    "The combination of SMOTE and ENN aims to balance the dataset by generating synthetic minority samples (using SMOTE) and cleaning the dataset by removing noisy, borderline instances (using ENN).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e5dcf-9856-4b65-9623-b64495ddc619",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>How SMOTEENN Works:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Generate Synthetic Minority Samples (SMOTE):</font>\n",
    "        <ol>The algorithm first applies SMOTE to create synthetic samples for the minority class. This increases the number of minority class samples by generating synthetic samples based on the nearest neighbors.</ol></li>\n",
    "    <li><font color=\"orange\">Remove Noisy and Borderline Samples (ENN):</font>\n",
    "        <ol>After generating synthetic samples, ENN is applied to remove noisy or borderline samples. It identifies samples whose neighbors belong to a different class and removes them from the dataset.</ol></li>\n",
    "    <li><font color=\"orange\">Balanced Dataset:</font>\n",
    "        <ol>The result is a dataset where the minority class is augmented with synthetic samples, and both classes are cleaned of noisy, ambiguous samples.</ol></li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd29ade0-4860-4f0e-9bf5-97cde7eaf672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTEENN: Counter({0: 898, 1: 102})\n",
      "Class distribution after SMOTEENN: Counter({1: 892, 0: 746})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       235\n",
      "           1       0.97      0.97      0.97       257\n",
      "\n",
      "    accuracy                           0.97       492\n",
      "   macro avg       0.97      0.97      0.97       492\n",
      "weighted avg       0.97      0.97      0.97       492\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying SMOTEENN\n",
    "print(\"Class distribution before SMOTEENN:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply SMOTEENN to balance the dataset\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the class distribution after applying SMOTEENN\n",
    "print(\"Class distribution after SMOTEENN:\", Counter(y_resampled))\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (RandomForest) on the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493a702f-4b52-4de0-91ca-c6e121a1d2d8",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Advantages of SMOTEENN:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Hybrid Approach:</font> SMOTEENN combines the benefits of SMOTE (over-sampling) and ENN (under-sampling), leading to a more balanced and cleaner dataset.</li>\n",
    "    <li><font color=\"orange\">Noise Reduction:</font> The ENN step helps reduce noisy or ambiguous samples, leading to a cleaner dataset and potentially better model performance.</li>\n",
    "    <li><font color=\"orange\">Improved Performance:</font> By both increasing the minority class size and removing noisy majority class samples, SMOTEENN can help improve the performance of classifiers on imbalanced datasets.</li></ol>\n",
    "\n",
    "<font color=\"pink\" size=4>Drawbacks of SMOTEENN:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Computational Complexity:</font> The combination of both over-sampling and under-sampling methods can be computationally expensive, especially for large datasets.</li>\n",
    "    <li><font color=\"orange\">Loss of Information:</font> The ENN step can lead to the removal of potentially useful majority class samples, which might impact the performance in some cases.</li>\n",
    "    <li><font color=\"orange\">Not Always Effective:</font> If the dataset doesn't have a significant amount of noise or borderline samples, SMOTEENN might not provide substantial benefits over simpler techniques like SMOTE or RandomUnderSampler.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dcfc6-e273-40bc-b0d5-09d7ee701c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
