{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb9098e-940c-401d-9bc5-9a88fd733828",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\"><b>Ensemble Methods</font>\n",
    "<p><font color=\"Yellow\" size=\"5\"><b>2_EasyEnsembleClassifier</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b8a226-e36a-4619-a81f-e2de0cf1a30b",
   "metadata": {},
   "source": [
    "The EasyEnsembleClassifier is a type of ensemble method based on under-sampling the majority class. It works by creating multiple subsets (or bags) from the minority class and then creating several versions of the dataset by combining each bag with a random under-sampled subset of the majority class. These subsets are then used to train different classifiers, and their predictions are combined to make the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e165c-c23f-4e33-83f3-afa919d469fa",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Steps in EasyEnsembleClassifier:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Under-sampling of Majority Class:</font> In each iteration, a random under-sample of the majority class is selected.</li>\n",
    "    <li><font color=\"orange\">Ensemble Creation:</font> Multiple subsets are created by combining each under-sampled majority class set with the original minority class.</li>\n",
    "    <li><font color=\"orange\">Model Training:</font> For each subset, a classifier is trained on the balanced dataset.</li>\n",
    "    <li><font color=\"orange\">Voting:</font> The final prediction is made using a majority vote (for classification) from all the individual classifiers in the ensemble.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8c0755-1bb7-4989-9acb-a6afd1f7f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before EasyEnsembleClassifier: Counter({0: 898, 1: 102})\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.95       275\n",
      "           1       0.46      0.84      0.59        25\n",
      "\n",
      "    accuracy                           0.90       300\n",
      "   macro avg       0.72      0.87      0.77       300\n",
      "weighted avg       0.94      0.90      0.92       300\n",
      "\n",
      "Class distribution after EasyEnsembleClassifier: Counter({0: 254, 1: 46})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying EasyEnsembleClassifier\n",
    "print(\"Class distribution before EasyEnsembleClassifier:\", Counter(y))\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Train the EasyEnsembleClassifier with the correct parameters\n",
    "clf = EasyEnsembleClassifier(n_estimators=10, random_state=42,sampling_strategy=.9)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the classifier\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Check the class distribution of the predictions\n",
    "print(\"Class distribution after EasyEnsembleClassifier:\", Counter(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12fa09-52a8-40ed-af3d-44c1fae9c9cf",
   "metadata": {},
   "source": [
    "<b><font color=\"sky blue\">The EasyEnsembleClassifier does not force an exact 50-50 distribution after resampling. Instead, it aims to balance the class distribution in a way that can improve the modelâ€™s performance on the minority class. The final distribution may not always be exactly even (i.e., 50-50) because the minority class is often much smaller to begin with, so it may still have a slight imbalance depending on how many estimators (models) you use.\n",
    "Why isn't the distribution exactly even?</font></b>\n",
    "<ol>\n",
    "    <li>The ensemble tries to balance each individual classifier by sampling a subset of the majority class, but it doesn't necessarily ensure an exactly balanced dataset across all classifiers. Instead, it just aims for a balanced performance by adjusting the majority class for each weak learner.</li>\n",
    "    <li>The class distribution after resampling depends on the number of estimators (n_estimators), the number of samples in the minority class, and the number of under-sampled majority class instances.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c4866-8535-4a69-a781-75d14f34b8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
