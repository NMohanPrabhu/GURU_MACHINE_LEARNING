{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87de458f-f249-47a3-8020-069a42f01bbb",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Ensemble methods</font>\n",
    "<p> <font color=\"Yellow\" size=\"5\"><b>5_CatBoost (Categorical Boosting)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1abec-c52a-4f43-9f68-1013878f264a",
   "metadata": {},
   "source": [
    "CatBoost (Categorical Boosting) is a gradient boosting algorithm developed by Yandex. It is designed to handle categorical features efficiently without needing to perform one-hot encoding or other preprocessing steps. CatBoost is a powerful tool for both classification and regression tasks, and it often performs well with minimal hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bca9fa-9e6e-4959-9d51-2b5fee125941",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Key Features of CatBoost:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Handling Categorical Features:</font> Unlike other gradient boosting algorithms, CatBoost can handle categorical features directly, automatically transforming them into numerical representations without the need for explicit encoding like one-hot encoding.</li>\n",
    "    <li><font color=\"orange\">Ordered Boosting:</font> CatBoost introduces an innovative technique called \"Ordered Boosting\" that helps reduce overfitting, especially in small datasets.</li>\n",
    "    <li><font color=\"orange\">Efficient and Fast:</font> CatBoost is highly optimized for both speed and memory efficiency, making it faster than other gradient boosting methods in some cases.</li>\n",
    "    <li><font color=\"orange\">Regularization:</font> It supports L2 regularization and incorporates methods to avoid overfitting during training.</li>\n",
    "    <li><font color=\"orange\">Supports Multiclass Classification:</font> CatBoost can be used for multiclass classification tasks with minimal configuration.</li>\n",
    "    <li><font color=\"orange\">Explanability:</font> It provides tools to interpret and visualize the feature importance and model behavior.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8dd231-0e2f-4909-bb1b-9b625e3f954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79acf5da-a0fe-4747-a3e9-f692ab519124",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>CatBoost Classifier:</font>\n",
    "<ol>\n",
    "     <li><font color=\"orange\">iterations:</font> The number of boosting iterations (trees). We set it to 1000 here, but you can adjust it.</li>\n",
    "     <li><font color=\"orange\">learning_rate:</font> The learning rate for each iteration, which determines the size of the steps taken towards minimizing the loss function.</li>\n",
    "     <li><font color=\"orange\">depth</font> The depth of the individual decision trees. Deeper trees can capture more complex patterns but might lead to overfitting.</li>\n",
    "     <li><font color=\"orange\">cat_features:</font> In this case, we donâ€™t have categorical features, but if you have categorical features, you can pass the column indices here.    </li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1cfac8-a7bf-43a6-89a3-f6d1d5225c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0171507\ttotal: 178ms\tremaining: 2m 58s\n",
      "200:\tlearn: 0.0194995\ttotal: 817ms\tremaining: 3.25s\n",
      "400:\tlearn: 0.0084179\ttotal: 1.37s\tremaining: 2.05s\n",
      "600:\tlearn: 0.0052795\ttotal: 1.93s\tremaining: 1.28s\n",
      "800:\tlearn: 0.0038484\ttotal: 2.46s\tremaining: 613ms\n",
      "999:\tlearn: 0.0030384\ttotal: 3s\tremaining: 0us\n",
      "Accuracy: 0.9815\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        19\n",
      "           1       1.00      0.95      0.98        21\n",
      "           2       1.00      1.00      1.00        14\n",
      "\n",
      "    accuracy                           0.98        54\n",
      "   macro avg       0.98      0.98      0.98        54\n",
      "weighted avg       0.98      0.98      0.98        54\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  0  0]\n",
      " [ 1 20  0]\n",
      " [ 0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 3. Create the CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000,        # Number of boosting iterations\n",
    "    learning_rate=0.1,      # Learning rate for each iteration\n",
    "    depth=6,                # Depth of each tree\n",
    "    random_state=42,        # Random seed for reproducibility\n",
    "    cat_features=[]         # No categorical features for this dataset, but this can be used for datasets with categorical columns\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "catboost_model.fit(X_train, y_train, verbose=200)  # Verbose outputs the training progress every 200 iterations\n",
    "\n",
    "# 5. Make predictions on the test set\n",
    "y_pred = catboost_model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display the classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b44f61-5083-47b8-87c1-0d7cfb1ad5f5",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Hyperparameters in CatBoost:</font>\n",
    "\n",
    "Here are some important hyperparameters that you can adjust to improve performance:\n",
    "<ol>\n",
    "    <li><font color=\"orange\">iterations:</font> The number of boosting rounds (trees). Higher values can lead to better performance but might increase the risk of overfitting.</li>\n",
    "    <li><font color=\"orange\">learning_rate:</font> The learning rate controls how much each tree contributes to the final prediction. Lower values typically lead to better generalization but require more iterations.</li>\n",
    "    <li><font color=\"orange\">depth:</font> The maximum depth of individual trees. Deeper trees can capture more complex patterns but may overfit the data.</li>\n",
    "    <li><font color=\"orange\">cat_features:</font> Indexes of categorical features in the dataset (not used in the Wine dataset, but can be helpful for datasets with categorical columns).</li>\n",
    "    <li><font color=\"orange\">subsample:</font> The fraction of samples used to train each tree. This can help prevent overfitting.</li>\n",
    "    <li><font color=\"orange\">reg_lambda:</font> L2 regularization coefficient. This helps prevent overfitting.</li>\n",
    "    <li><font color=\"orange\">border_count:</font> The number of discrete values for numeric features. This is used when CatBoost transforms numeric features into categorical features internally. </li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e664d6-19df-468d-953a-f7517b56fae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
