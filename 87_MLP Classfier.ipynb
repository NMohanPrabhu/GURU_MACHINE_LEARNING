{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585bb375-fa1b-4ca9-8b36-14fb2355819e",
   "metadata": {},
   "source": [
    "<font color=\"Yellow\" size=\"6\">Neural Networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44689a-23e0-47ab-b538-0a71b9624ce9",
   "metadata": {},
   "source": [
    "<font color=\"yellow\" size=\"6\">1_MLP Classfier </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d480f64-781b-42b3-aeab-910c91bd96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eeee6af-64a1-4f49-ad53-168e1e3b056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data  # Features\n",
    "y = wine.target  # Target variable (Wine classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e75fa0-60ad-4e25-b5f7-11e17a994d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7bf41-489c-466c-8091-3e71f0a8cddd",
   "metadata": {},
   "source": [
    "The model is defined with three hidden layers: (100, 50, 25). This means there are three layers with 100, 50, and 25 neurons, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5958384a-fb07-4d40-9a6b-8e20e40521da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLP classifier with multiple hidden layers\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea15061b-b5df-4569-baea-96ea671f04b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 25.56306653\n",
      "Iteration 2, loss = 24.50927266\n",
      "Iteration 3, loss = 21.95727425\n",
      "Iteration 4, loss = 17.02482570\n",
      "Iteration 5, loss = 9.35145411\n",
      "Iteration 6, loss = 2.38480706\n",
      "Iteration 7, loss = 1.69791127\n",
      "Iteration 8, loss = 2.13035062\n",
      "Iteration 9, loss = 2.15740903\n",
      "Iteration 10, loss = 1.76023839\n",
      "Iteration 11, loss = 1.26132785\n",
      "Iteration 12, loss = 0.96682441\n",
      "Iteration 13, loss = 1.43518559\n",
      "Iteration 14, loss = 1.29014424\n",
      "Iteration 15, loss = 0.79081092\n",
      "Iteration 16, loss = 1.13607821\n",
      "Iteration 17, loss = 1.29708459\n",
      "Iteration 18, loss = 1.04543387\n",
      "Iteration 19, loss = 0.77002258\n",
      "Iteration 20, loss = 0.92060863\n",
      "Iteration 21, loss = 1.11775423\n",
      "Iteration 22, loss = 0.98973286\n",
      "Iteration 23, loss = 0.74583900\n",
      "Iteration 24, loss = 0.80617699\n",
      "Iteration 25, loss = 0.95070735\n",
      "Iteration 26, loss = 0.84906227\n",
      "Iteration 27, loss = 0.69024214\n",
      "Iteration 28, loss = 0.83152826\n",
      "Iteration 29, loss = 0.86878696\n",
      "Iteration 30, loss = 0.70550380\n",
      "Iteration 31, loss = 0.71866932\n",
      "Iteration 32, loss = 0.80849670\n",
      "Iteration 33, loss = 0.76040458\n",
      "Iteration 34, loss = 0.69200068\n",
      "Iteration 35, loss = 0.74250997\n",
      "Iteration 36, loss = 0.76641522\n",
      "Iteration 37, loss = 0.69189596\n",
      "Iteration 38, loss = 0.67948940\n",
      "Iteration 39, loss = 0.73141672\n",
      "Iteration 40, loss = 0.70269133\n",
      "Iteration 41, loss = 0.66813808\n",
      "Iteration 42, loss = 0.70685170\n",
      "Iteration 43, loss = 0.69212913\n",
      "Iteration 44, loss = 0.65521794\n",
      "Iteration 45, loss = 0.67832523\n",
      "Iteration 46, loss = 0.68355582\n",
      "Iteration 47, loss = 0.65660082\n",
      "Iteration 48, loss = 0.66212385\n",
      "Iteration 49, loss = 0.67145430\n",
      "Iteration 50, loss = 0.64818898\n",
      "Iteration 51, loss = 0.64570273\n",
      "Iteration 52, loss = 0.65787964\n",
      "Iteration 53, loss = 0.64310968\n",
      "Iteration 54, loss = 0.63968127\n",
      "Iteration 55, loss = 0.64716511\n",
      "Iteration 56, loss = 0.63323124\n",
      "Iteration 57, loss = 0.63025302\n",
      "Iteration 58, loss = 0.63558738\n",
      "Iteration 59, loss = 0.62615402\n",
      "Iteration 60, loss = 0.62360039\n",
      "Iteration 61, loss = 0.62591701\n",
      "Iteration 62, loss = 0.61666614\n",
      "Iteration 63, loss = 0.61468959\n",
      "Iteration 64, loss = 0.61625113\n",
      "Iteration 65, loss = 0.60914151\n",
      "Iteration 66, loss = 0.60927643\n",
      "Iteration 67, loss = 0.60718944\n",
      "Iteration 68, loss = 0.60066299\n",
      "Iteration 69, loss = 0.60161297\n",
      "Iteration 70, loss = 0.59818694\n",
      "Iteration 71, loss = 0.59524508\n",
      "Iteration 72, loss = 0.59554597\n",
      "Iteration 73, loss = 0.59073105\n",
      "Iteration 74, loss = 0.58921471\n",
      "Iteration 75, loss = 0.58772569\n",
      "Iteration 76, loss = 0.58397467\n",
      "Iteration 77, loss = 0.58344821\n",
      "Iteration 78, loss = 0.58032462\n",
      "Iteration 79, loss = 0.57789632\n",
      "Iteration 80, loss = 0.57668832\n",
      "Iteration 81, loss = 0.57363862\n",
      "Iteration 82, loss = 0.57243367\n",
      "Iteration 83, loss = 0.57021222\n",
      "Iteration 84, loss = 0.56773014\n",
      "Iteration 85, loss = 0.56621306\n",
      "Iteration 86, loss = 0.56371102\n",
      "Iteration 87, loss = 0.56201582\n",
      "Iteration 88, loss = 0.56011507\n",
      "Iteration 89, loss = 0.55786688\n",
      "Iteration 90, loss = 0.55626960\n",
      "Iteration 91, loss = 0.55407155\n",
      "Iteration 92, loss = 0.55237301\n",
      "Iteration 93, loss = 0.55056488\n",
      "Iteration 94, loss = 0.54851245\n",
      "Iteration 95, loss = 0.54684556\n",
      "Iteration 96, loss = 0.54481127\n",
      "Iteration 97, loss = 0.54310066\n",
      "Iteration 98, loss = 0.54132224\n",
      "Iteration 99, loss = 0.53943798\n",
      "Iteration 100, loss = 0.53774384\n",
      "Iteration 101, loss = 0.53581157\n",
      "Iteration 102, loss = 0.53414342\n",
      "Iteration 103, loss = 0.53242117\n",
      "Iteration 104, loss = 0.53056121\n",
      "Iteration 105, loss = 0.52885860\n",
      "Iteration 106, loss = 0.52710265\n",
      "Iteration 107, loss = 0.52538902\n",
      "Iteration 108, loss = 0.52364409\n",
      "Iteration 109, loss = 0.52194255\n",
      "Iteration 110, loss = 0.52026080\n",
      "Iteration 111, loss = 0.51852615\n",
      "Iteration 112, loss = 0.51685681\n",
      "Iteration 113, loss = 0.51514742\n",
      "Iteration 114, loss = 0.51350871\n",
      "Iteration 115, loss = 0.51179621\n",
      "Iteration 116, loss = 0.51015569\n",
      "Iteration 117, loss = 0.50847335\n",
      "Iteration 118, loss = 0.50681200\n",
      "Iteration 119, loss = 0.50517806\n",
      "Iteration 120, loss = 0.50350260\n",
      "Iteration 121, loss = 0.50186819\n",
      "Iteration 122, loss = 0.50023839\n",
      "Iteration 123, loss = 0.49861568\n",
      "Iteration 124, loss = 0.49696896\n",
      "Iteration 125, loss = 0.49533549\n",
      "Iteration 126, loss = 0.49372603\n",
      "Iteration 127, loss = 0.49211030\n",
      "Iteration 128, loss = 0.49049979\n",
      "Iteration 129, loss = 0.48889046\n",
      "Iteration 130, loss = 0.48730545\n",
      "Iteration 131, loss = 0.48569343\n",
      "Iteration 132, loss = 0.48410651\n",
      "Iteration 133, loss = 0.48244560\n",
      "Iteration 134, loss = 0.48070610\n",
      "Iteration 135, loss = 0.47901705\n",
      "Iteration 136, loss = 0.47750035\n",
      "Iteration 137, loss = 0.47595778\n",
      "Iteration 138, loss = 0.47435715\n",
      "Iteration 139, loss = 0.47269852\n",
      "Iteration 140, loss = 0.47099408\n",
      "Iteration 141, loss = 0.46951144\n",
      "Iteration 142, loss = 0.46779819\n",
      "Iteration 143, loss = 0.46513688\n",
      "Iteration 144, loss = 0.46386471\n",
      "Iteration 145, loss = 0.46220702\n",
      "Iteration 146, loss = 0.46056619\n",
      "Iteration 147, loss = 0.45865827\n",
      "Iteration 148, loss = 0.45695279\n",
      "Iteration 149, loss = 0.45512722\n",
      "Iteration 150, loss = 0.45342066\n",
      "Iteration 151, loss = 0.45168117\n",
      "Iteration 152, loss = 0.44999534\n",
      "Iteration 153, loss = 0.44795658\n",
      "Iteration 154, loss = 0.44627465\n",
      "Iteration 155, loss = 0.44444032\n",
      "Iteration 156, loss = 0.44256983\n",
      "Iteration 157, loss = 0.44088287\n",
      "Iteration 158, loss = 0.43894700\n",
      "Iteration 159, loss = 0.43712317\n",
      "Iteration 160, loss = 0.43536155\n",
      "Iteration 161, loss = 0.43359757\n",
      "Iteration 162, loss = 0.43172202\n",
      "Iteration 163, loss = 0.42978902\n",
      "Iteration 164, loss = 0.42783627\n",
      "Iteration 165, loss = 0.42579594\n",
      "Iteration 166, loss = 0.42434586\n",
      "Iteration 167, loss = 0.42243833\n",
      "Iteration 168, loss = 0.42030021\n",
      "Iteration 169, loss = 0.41847420\n",
      "Iteration 170, loss = 0.41673537\n",
      "Iteration 171, loss = 0.41483726\n",
      "Iteration 172, loss = 0.41327863\n",
      "Iteration 173, loss = 0.41119312\n",
      "Iteration 174, loss = 0.40938225\n",
      "Iteration 175, loss = 0.40759320\n",
      "Iteration 176, loss = 0.40538761\n",
      "Iteration 177, loss = 0.40345915\n",
      "Iteration 178, loss = 0.40141891\n",
      "Iteration 179, loss = 0.39970772\n",
      "Iteration 180, loss = 0.39794316\n",
      "Iteration 181, loss = 0.39582940\n",
      "Iteration 182, loss = 0.39387370\n",
      "Iteration 183, loss = 0.39209647\n",
      "Iteration 184, loss = 0.39022082\n",
      "Iteration 185, loss = 0.38829398\n",
      "Iteration 186, loss = 0.38584435\n",
      "Iteration 187, loss = 0.38290970\n",
      "Iteration 188, loss = 0.38035488\n",
      "Iteration 189, loss = 0.37762034\n",
      "Iteration 190, loss = 0.37524687\n",
      "Iteration 191, loss = 0.37281983\n",
      "Iteration 192, loss = 0.36979248\n",
      "Iteration 193, loss = 0.36711978\n",
      "Iteration 194, loss = 0.36492752\n",
      "Iteration 195, loss = 0.36308024\n",
      "Iteration 196, loss = 0.36019817\n",
      "Iteration 197, loss = 0.35791166\n",
      "Iteration 198, loss = 0.35587212\n",
      "Iteration 199, loss = 0.35365045\n",
      "Iteration 200, loss = 0.35119093\n",
      "Iteration 201, loss = 0.34893984\n",
      "Iteration 202, loss = 0.34690731\n",
      "Iteration 203, loss = 0.34472786\n",
      "Iteration 204, loss = 0.34235280\n",
      "Iteration 205, loss = 0.34009962\n",
      "Iteration 206, loss = 0.33798089\n",
      "Iteration 207, loss = 0.33577119\n",
      "Iteration 208, loss = 0.33347975\n",
      "Iteration 209, loss = 0.33145258\n",
      "Iteration 210, loss = 0.32940666\n",
      "Iteration 211, loss = 0.32744128\n",
      "Iteration 212, loss = 0.32542629\n",
      "Iteration 213, loss = 0.32336120\n",
      "Iteration 214, loss = 0.32137428\n",
      "Iteration 215, loss = 0.31945839\n",
      "Iteration 216, loss = 0.31754155\n",
      "Iteration 217, loss = 0.31562680\n",
      "Iteration 218, loss = 0.31355891\n",
      "Iteration 219, loss = 0.31153615\n",
      "Iteration 220, loss = 0.30962115\n",
      "Iteration 221, loss = 0.30818645\n",
      "Iteration 222, loss = 0.30614781\n",
      "Iteration 223, loss = 0.30423716\n",
      "Iteration 224, loss = 0.30254972\n",
      "Iteration 225, loss = 0.30083284\n",
      "Iteration 226, loss = 0.29909479\n",
      "Iteration 227, loss = 0.29734925\n",
      "Iteration 228, loss = 0.29561254\n",
      "Iteration 229, loss = 0.29389559\n",
      "Iteration 230, loss = 0.29219982\n",
      "Iteration 231, loss = 0.29051932\n",
      "Iteration 232, loss = 0.28885504\n",
      "Iteration 233, loss = 0.28721341\n",
      "Iteration 234, loss = 0.28559964\n",
      "Iteration 235, loss = 0.28401793\n",
      "Iteration 236, loss = 0.28263135\n",
      "Iteration 237, loss = 0.28101809\n",
      "Iteration 238, loss = 0.27953276\n",
      "Iteration 239, loss = 0.27796375\n",
      "Iteration 240, loss = 0.27634699\n",
      "Iteration 241, loss = 0.27476394\n",
      "Iteration 242, loss = 0.27315554\n",
      "Iteration 243, loss = 0.27173278\n",
      "Iteration 244, loss = 0.27041640\n",
      "Iteration 245, loss = 0.26911298\n",
      "Iteration 246, loss = 0.26774190\n",
      "Iteration 247, loss = 0.26625073\n",
      "Iteration 248, loss = 0.26465914\n",
      "Iteration 249, loss = 0.26309025\n",
      "Iteration 250, loss = 0.26165600\n",
      "Iteration 251, loss = 0.26032762\n",
      "Iteration 252, loss = 0.25898243\n",
      "Iteration 253, loss = 0.25755187\n",
      "Iteration 254, loss = 0.25601957\n",
      "Iteration 255, loss = 0.25444999\n",
      "Iteration 256, loss = 0.25294174\n",
      "Iteration 257, loss = 0.25176617\n",
      "Iteration 258, loss = 0.25026216\n",
      "Iteration 259, loss = 0.24898008\n",
      "Iteration 260, loss = 0.24766046\n",
      "Iteration 261, loss = 0.24616106\n",
      "Iteration 262, loss = 0.24457209\n",
      "Iteration 263, loss = 0.24304127\n",
      "Iteration 264, loss = 0.24161053\n",
      "Iteration 265, loss = 0.24031380\n",
      "Iteration 266, loss = 0.23904398\n",
      "Iteration 267, loss = 0.23779241\n",
      "Iteration 268, loss = 0.23663514\n",
      "Iteration 269, loss = 0.23509458\n",
      "Iteration 270, loss = 0.23341479\n",
      "Iteration 271, loss = 0.23182799\n",
      "Iteration 272, loss = 0.23050314\n",
      "Iteration 273, loss = 0.22926463\n",
      "Iteration 274, loss = 0.22785411\n",
      "Iteration 275, loss = 0.22643875\n",
      "Iteration 276, loss = 0.22513564\n",
      "Iteration 277, loss = 0.22380910\n",
      "Iteration 278, loss = 0.22246748\n",
      "Iteration 279, loss = 0.22114021\n",
      "Iteration 280, loss = 0.21974015\n",
      "Iteration 281, loss = 0.21846247\n",
      "Iteration 282, loss = 0.21724706\n",
      "Iteration 283, loss = 0.21599649\n",
      "Iteration 284, loss = 0.21464081\n",
      "Iteration 285, loss = 0.21390150\n",
      "Iteration 286, loss = 0.21265666\n",
      "Iteration 287, loss = 0.21146224\n",
      "Iteration 288, loss = 0.20984246\n",
      "Iteration 289, loss = 0.20846358\n",
      "Iteration 290, loss = 0.20717143\n",
      "Iteration 291, loss = 0.20610872\n",
      "Iteration 292, loss = 0.20512307\n",
      "Iteration 293, loss = 0.20383195\n",
      "Iteration 294, loss = 0.20258583\n",
      "Iteration 295, loss = 0.20129926\n",
      "Iteration 296, loss = 0.20015847\n",
      "Iteration 297, loss = 0.20244883\n",
      "Iteration 298, loss = 0.22422509\n",
      "Iteration 299, loss = 0.21535782\n",
      "Iteration 300, loss = 0.20194006\n",
      "Iteration 301, loss = 0.21619444\n",
      "Iteration 302, loss = 0.19548360\n",
      "Iteration 303, loss = 0.21058784\n",
      "Iteration 304, loss = 0.19267401\n",
      "Iteration 305, loss = 0.20670978\n",
      "Iteration 306, loss = 0.19179206\n",
      "Iteration 307, loss = 0.20090417\n",
      "Iteration 308, loss = 0.19060566\n",
      "Iteration 309, loss = 0.19760306\n",
      "Iteration 310, loss = 0.18928888\n",
      "Iteration 311, loss = 0.19380020\n",
      "Iteration 312, loss = 0.18795836\n",
      "Iteration 313, loss = 0.19117394\n",
      "Iteration 314, loss = 0.18638607\n",
      "Iteration 315, loss = 0.18847061\n",
      "Iteration 316, loss = 0.18476096\n",
      "Iteration 317, loss = 0.18650416\n",
      "Iteration 318, loss = 0.18295839\n",
      "Iteration 319, loss = 0.18454112\n",
      "Iteration 320, loss = 0.18159565\n",
      "Iteration 321, loss = 0.18255288\n",
      "Iteration 322, loss = 0.18002628\n",
      "Iteration 323, loss = 0.18085719\n",
      "Iteration 324, loss = 0.17862534\n",
      "Iteration 325, loss = 0.17917029\n",
      "Iteration 326, loss = 0.17717811\n",
      "Iteration 327, loss = 0.17769130\n",
      "Iteration 328, loss = 0.17597365\n",
      "Iteration 329, loss = 0.17610356\n",
      "Iteration 330, loss = 0.17468699\n",
      "Iteration 331, loss = 0.17485463\n",
      "Iteration 332, loss = 0.17339059\n",
      "Iteration 333, loss = 0.17326543\n",
      "Iteration 334, loss = 0.17218973\n",
      "Iteration 335, loss = 0.17198863\n",
      "Iteration 336, loss = 0.17096910\n",
      "Iteration 337, loss = 0.17071407\n",
      "Iteration 338, loss = 0.16980117\n",
      "Iteration 339, loss = 0.16942687\n",
      "Iteration 340, loss = 0.16861715\n",
      "Iteration 341, loss = 0.16823847\n",
      "Iteration 342, loss = 0.16757098\n",
      "Iteration 343, loss = 0.16707188\n",
      "Iteration 344, loss = 0.16640363\n",
      "Iteration 345, loss = 0.16588742\n",
      "Iteration 346, loss = 0.16529564\n",
      "Iteration 347, loss = 0.16476437\n",
      "Iteration 348, loss = 0.16420804\n",
      "Iteration 349, loss = 0.16366436\n",
      "Iteration 350, loss = 0.16312428\n",
      "Iteration 351, loss = 0.16254388\n",
      "Iteration 352, loss = 0.16205232\n",
      "Iteration 353, loss = 0.16146099\n",
      "Iteration 354, loss = 0.16099603\n",
      "Iteration 355, loss = 0.16040027\n",
      "Iteration 356, loss = 0.15998457\n",
      "Iteration 357, loss = 0.15939173\n",
      "Iteration 358, loss = 0.15894229\n",
      "Iteration 359, loss = 0.15839774\n",
      "Iteration 360, loss = 0.15793489\n",
      "Iteration 361, loss = 0.15740154\n",
      "Iteration 362, loss = 0.15690375\n",
      "Iteration 363, loss = 0.15640654\n",
      "Iteration 364, loss = 0.15591433\n",
      "Iteration 365, loss = 0.15541081\n",
      "Iteration 366, loss = 0.15492127\n",
      "Iteration 367, loss = 0.15442458\n",
      "Iteration 368, loss = 0.15393178\n",
      "Iteration 369, loss = 0.15343768\n",
      "Iteration 370, loss = 0.15295483\n",
      "Iteration 371, loss = 0.15247048\n",
      "Iteration 372, loss = 0.15201752\n",
      "Iteration 373, loss = 0.15151795\n",
      "Iteration 374, loss = 0.15104384\n",
      "Iteration 375, loss = 0.15057745\n",
      "Iteration 376, loss = 0.15011088\n",
      "Iteration 377, loss = 0.14964776\n",
      "Iteration 378, loss = 0.14919318\n",
      "Iteration 379, loss = 0.14873077\n",
      "Iteration 380, loss = 0.14828001\n",
      "Iteration 381, loss = 0.14783309\n",
      "Iteration 382, loss = 0.14739465\n",
      "Iteration 383, loss = 0.14695980\n",
      "Iteration 384, loss = 0.14652356\n",
      "Iteration 385, loss = 0.14609859\n",
      "Iteration 386, loss = 0.14568316\n",
      "Iteration 387, loss = 0.14526970\n",
      "Iteration 388, loss = 0.14486733\n",
      "Iteration 389, loss = 0.14446843\n",
      "Iteration 390, loss = 0.14407073\n",
      "Iteration 391, loss = 0.14367585\n",
      "Iteration 392, loss = 0.14328293\n",
      "Iteration 393, loss = 0.14289336\n",
      "Iteration 394, loss = 0.14250610\n",
      "Iteration 395, loss = 0.14213535\n",
      "Iteration 396, loss = 0.14176853\n",
      "Iteration 397, loss = 0.14139304\n",
      "Iteration 398, loss = 0.14102490\n",
      "Iteration 399, loss = 0.14066022\n",
      "Iteration 400, loss = 0.14029102\n",
      "Iteration 401, loss = 0.13991915\n",
      "Iteration 402, loss = 0.13955532\n",
      "Iteration 403, loss = 0.13919757\n",
      "Iteration 404, loss = 0.13883600\n",
      "Iteration 405, loss = 0.13847612\n",
      "Iteration 406, loss = 0.13811386\n",
      "Iteration 407, loss = 0.13775987\n",
      "Iteration 408, loss = 0.13739212\n",
      "Iteration 409, loss = 0.13704102\n",
      "Iteration 410, loss = 0.13669310\n",
      "Iteration 411, loss = 0.13636746\n",
      "Iteration 412, loss = 0.13598731\n",
      "Iteration 413, loss = 0.13565540\n",
      "Iteration 414, loss = 0.13530760\n",
      "Iteration 415, loss = 0.13497617\n",
      "Iteration 416, loss = 0.13466173\n",
      "Iteration 417, loss = 0.13429644\n",
      "Iteration 418, loss = 0.13398528\n",
      "Iteration 419, loss = 0.13364352\n",
      "Iteration 420, loss = 0.13330979\n",
      "Iteration 421, loss = 0.13299224\n",
      "Iteration 422, loss = 0.13266432\n",
      "Iteration 423, loss = 0.13233225\n",
      "Iteration 424, loss = 0.13209460\n",
      "Iteration 425, loss = 0.13172199\n",
      "Iteration 426, loss = 0.13141148\n",
      "Iteration 427, loss = 0.13108647\n",
      "Iteration 428, loss = 0.13079085\n",
      "Iteration 429, loss = 0.13048040\n",
      "Iteration 430, loss = 0.13018147\n",
      "Iteration 431, loss = 0.12986486\n",
      "Iteration 432, loss = 0.12955790\n",
      "Iteration 433, loss = 0.12926932\n",
      "Iteration 434, loss = 0.12896545\n",
      "Iteration 435, loss = 0.12867692\n",
      "Iteration 436, loss = 0.12836901\n",
      "Iteration 437, loss = 0.12808353\n",
      "Iteration 438, loss = 0.12777265\n",
      "Iteration 439, loss = 0.12748654\n",
      "Iteration 440, loss = 0.12760958\n",
      "Iteration 441, loss = 0.12743619\n",
      "Iteration 442, loss = 0.12680996\n",
      "Iteration 443, loss = 0.12660379\n",
      "Iteration 444, loss = 0.12626354\n",
      "Iteration 445, loss = 0.12596792\n",
      "Iteration 446, loss = 0.12572131\n",
      "Iteration 447, loss = 0.12519742\n",
      "Iteration 448, loss = 0.12515161\n",
      "Iteration 449, loss = 0.12473074\n",
      "Iteration 450, loss = 0.12448736\n",
      "Iteration 451, loss = 0.12419913\n",
      "Iteration 452, loss = 0.12386883\n",
      "Iteration 453, loss = 0.12367477\n",
      "Iteration 454, loss = 0.12331961\n",
      "Iteration 455, loss = 0.12308007\n",
      "Iteration 456, loss = 0.12278531\n",
      "Iteration 457, loss = 0.12246655\n",
      "Iteration 458, loss = 0.12223371\n",
      "Iteration 459, loss = 0.12192564\n",
      "Iteration 460, loss = 0.12169494\n",
      "Iteration 461, loss = 0.12140958\n",
      "Iteration 462, loss = 0.12112422\n",
      "Iteration 463, loss = 0.12085537\n",
      "Iteration 464, loss = 0.12059654\n",
      "Iteration 465, loss = 0.12032085\n",
      "Iteration 466, loss = 0.12004235\n",
      "Iteration 467, loss = 0.11974299\n",
      "Iteration 468, loss = 0.11944673\n",
      "Iteration 469, loss = 0.11918839\n",
      "Iteration 470, loss = 0.11903192\n",
      "Iteration 471, loss = 0.11900380\n",
      "Iteration 472, loss = 0.11859141\n",
      "Iteration 473, loss = 0.11803186\n",
      "Iteration 474, loss = 0.11797709\n",
      "Iteration 475, loss = 0.11778359\n",
      "Iteration 476, loss = 0.11721465\n",
      "Iteration 477, loss = 0.11692273\n",
      "Iteration 478, loss = 0.11674751\n",
      "Iteration 479, loss = 0.11639751\n",
      "Iteration 480, loss = 0.11604996\n",
      "Iteration 481, loss = 0.11574471\n",
      "Iteration 482, loss = 0.11549372\n",
      "Iteration 483, loss = 0.11520460\n",
      "Iteration 484, loss = 0.11483361\n",
      "Iteration 485, loss = 0.11455135\n",
      "Iteration 486, loss = 0.11433013\n",
      "Iteration 487, loss = 0.11421402\n",
      "Iteration 488, loss = 0.11398042\n",
      "Iteration 489, loss = 0.11351914\n",
      "Iteration 490, loss = 0.11302698\n",
      "Iteration 491, loss = 0.11268131\n",
      "Iteration 492, loss = 0.11248896\n",
      "Iteration 493, loss = 0.11227569\n",
      "Iteration 494, loss = 0.11201504\n",
      "Iteration 495, loss = 0.11159063\n",
      "Iteration 496, loss = 0.11120397\n",
      "Iteration 497, loss = 0.11082832\n",
      "Iteration 498, loss = 0.11049304\n",
      "Iteration 499, loss = 0.11019672\n",
      "Iteration 500, loss = 0.11002112\n",
      "Iteration 501, loss = 0.11027727\n",
      "Iteration 502, loss = 0.11078339\n",
      "Iteration 503, loss = 0.11054342\n",
      "Iteration 504, loss = 0.10901408\n",
      "Iteration 505, loss = 0.10852668\n",
      "Iteration 506, loss = 0.10894472\n",
      "Iteration 507, loss = 0.10840347\n",
      "Iteration 508, loss = 0.10756341\n",
      "Iteration 509, loss = 0.10767312\n",
      "Iteration 510, loss = 0.10774055\n",
      "Iteration 511, loss = 0.10688491\n",
      "Iteration 512, loss = 0.10646097\n",
      "Iteration 513, loss = 0.10682784\n",
      "Iteration 514, loss = 0.10651143\n",
      "Iteration 515, loss = 0.10568423\n",
      "Iteration 516, loss = 0.10546812\n",
      "Iteration 517, loss = 0.10547790\n",
      "Iteration 518, loss = 0.10496460\n",
      "Iteration 519, loss = 0.10454305\n",
      "Iteration 520, loss = 0.10443324\n",
      "Iteration 521, loss = 0.10429729\n",
      "Iteration 522, loss = 0.10384921\n",
      "Iteration 523, loss = 0.10345619\n",
      "Iteration 524, loss = 0.10330578\n",
      "Iteration 525, loss = 0.10303687\n",
      "Iteration 526, loss = 0.10268452\n",
      "Iteration 527, loss = 0.10237758\n",
      "Iteration 528, loss = 0.10216073\n",
      "Iteration 529, loss = 0.10201134\n",
      "Iteration 530, loss = 0.10169924\n",
      "Iteration 531, loss = 0.10128294\n",
      "Iteration 532, loss = 0.10107460\n",
      "Iteration 533, loss = 0.10089154\n",
      "Iteration 534, loss = 0.10064173\n",
      "Iteration 535, loss = 0.10028731\n",
      "Iteration 536, loss = 0.09994474\n",
      "Iteration 537, loss = 0.09972481\n",
      "Iteration 538, loss = 0.09944496\n",
      "Iteration 539, loss = 0.09916483\n",
      "Iteration 540, loss = 0.09890544\n",
      "Iteration 541, loss = 0.09866684\n",
      "Iteration 542, loss = 0.09841038\n",
      "Iteration 543, loss = 0.09814843\n",
      "Iteration 544, loss = 0.09789761\n",
      "Iteration 545, loss = 0.09768948\n",
      "Iteration 546, loss = 0.09743662\n",
      "Iteration 547, loss = 0.09725081\n",
      "Iteration 548, loss = 0.09709289\n",
      "Iteration 549, loss = 0.09696165\n",
      "Iteration 550, loss = 0.09683486\n",
      "Iteration 551, loss = 0.09674103\n",
      "Iteration 552, loss = 0.09656909\n",
      "Iteration 553, loss = 0.09623441\n",
      "Iteration 554, loss = 0.09593162\n",
      "Iteration 555, loss = 0.09543553\n",
      "Iteration 556, loss = 0.09500341\n",
      "Iteration 557, loss = 0.09475348\n",
      "Iteration 558, loss = 0.09477639\n",
      "Iteration 559, loss = 0.09486092\n",
      "Iteration 560, loss = 0.09460093\n",
      "Iteration 561, loss = 0.09417959\n",
      "Iteration 562, loss = 0.09374121\n",
      "Iteration 563, loss = 0.09337138\n",
      "Iteration 564, loss = 0.09310088\n",
      "Iteration 565, loss = 0.09292475\n",
      "Iteration 566, loss = 0.09285142\n",
      "Iteration 567, loss = 0.09269709\n",
      "Iteration 568, loss = 0.09239499\n",
      "Iteration 569, loss = 0.09202802\n",
      "Iteration 570, loss = 0.09174319\n",
      "Iteration 571, loss = 0.09156274\n",
      "Iteration 572, loss = 0.09147460\n",
      "Iteration 573, loss = 0.09140805\n",
      "Iteration 574, loss = 0.09130526\n",
      "Iteration 575, loss = 0.09105808\n",
      "Iteration 576, loss = 0.09065543\n",
      "Iteration 577, loss = 0.09029063\n",
      "Iteration 578, loss = 0.09003146\n",
      "Iteration 579, loss = 0.08989560\n",
      "Iteration 580, loss = 0.08989788\n",
      "Iteration 581, loss = 0.08988363\n",
      "Iteration 582, loss = 0.08971147\n",
      "Iteration 583, loss = 0.08933398\n",
      "Iteration 584, loss = 0.08891428\n",
      "Iteration 585, loss = 0.08858486\n",
      "Iteration 586, loss = 0.08846821\n",
      "Iteration 587, loss = 0.08850375\n",
      "Iteration 588, loss = 0.08859338\n",
      "Iteration 589, loss = 0.08844943\n",
      "Iteration 590, loss = 0.08802054\n",
      "Iteration 591, loss = 0.08751620\n",
      "Iteration 592, loss = 0.08718627\n",
      "Iteration 593, loss = 0.08715708\n",
      "Iteration 594, loss = 0.08731255\n",
      "Iteration 595, loss = 0.08736482\n",
      "Iteration 596, loss = 0.08705105\n",
      "Iteration 597, loss = 0.08646871\n",
      "Iteration 598, loss = 0.08604892\n",
      "Iteration 599, loss = 0.08594200\n",
      "Iteration 600, loss = 0.08600185\n",
      "Iteration 601, loss = 0.08609012\n",
      "Iteration 602, loss = 0.08591220\n",
      "Iteration 603, loss = 0.08542263\n",
      "Iteration 604, loss = 0.08496923\n",
      "Iteration 605, loss = 0.08479423\n",
      "Iteration 606, loss = 0.08490865\n",
      "Iteration 607, loss = 0.08499453\n",
      "Iteration 608, loss = 0.08473281\n",
      "Iteration 609, loss = 0.08423696\n",
      "Iteration 610, loss = 0.08384925\n",
      "Iteration 611, loss = 0.08375594\n",
      "Iteration 612, loss = 0.08384839\n",
      "Iteration 613, loss = 0.08386974\n",
      "Iteration 614, loss = 0.08360023\n",
      "Iteration 615, loss = 0.08315923\n",
      "Iteration 616, loss = 0.08280293\n",
      "Iteration 617, loss = 0.08272647\n",
      "Iteration 618, loss = 0.08277566\n",
      "Iteration 619, loss = 0.08278083\n",
      "Iteration 620, loss = 0.08250780\n",
      "Iteration 621, loss = 0.08215557\n",
      "Iteration 622, loss = 0.08186541\n",
      "Iteration 623, loss = 0.08165316\n",
      "Iteration 624, loss = 0.08149214\n",
      "Iteration 625, loss = 0.08134828\n",
      "Iteration 626, loss = 0.08124845\n",
      "Iteration 627, loss = 0.08116348\n",
      "Iteration 628, loss = 0.08101625\n",
      "Iteration 629, loss = 0.08081255\n",
      "Iteration 630, loss = 0.08054449\n",
      "Iteration 631, loss = 0.08032992\n",
      "Iteration 632, loss = 0.08014593\n",
      "Iteration 633, loss = 0.08000174\n",
      "Iteration 634, loss = 0.07990393\n",
      "Iteration 635, loss = 0.07978247\n",
      "Iteration 636, loss = 0.07967106\n",
      "Iteration 637, loss = 0.07948187\n",
      "Iteration 638, loss = 0.07922304\n",
      "Iteration 639, loss = 0.07897840\n",
      "Iteration 640, loss = 0.07880725\n",
      "Iteration 641, loss = 0.07869341\n",
      "Iteration 642, loss = 0.07858747\n",
      "Iteration 643, loss = 0.07845525\n",
      "Iteration 644, loss = 0.07828147\n",
      "Iteration 645, loss = 0.07807664\n",
      "Iteration 646, loss = 0.07786845\n",
      "Iteration 647, loss = 0.07768613\n",
      "Iteration 648, loss = 0.07753492\n",
      "Iteration 649, loss = 0.07740078\n",
      "Iteration 650, loss = 0.07726943\n",
      "Iteration 651, loss = 0.07715031\n",
      "Iteration 652, loss = 0.07709124\n",
      "Iteration 653, loss = 0.07704106\n",
      "Iteration 654, loss = 0.07700933\n",
      "Iteration 655, loss = 0.07687000\n",
      "Iteration 656, loss = 0.07661002\n",
      "Iteration 657, loss = 0.07638290\n",
      "Iteration 658, loss = 0.07635383\n",
      "Iteration 659, loss = 0.07649933\n",
      "Iteration 660, loss = 0.07695568\n",
      "Iteration 661, loss = 0.07713752\n",
      "Iteration 662, loss = 0.07681855\n",
      "Iteration 663, loss = 0.07590506\n",
      "Iteration 664, loss = 0.07519552\n",
      "Iteration 665, loss = 0.07500586\n",
      "Iteration 666, loss = 0.07520740\n",
      "Iteration 667, loss = 0.07557222\n",
      "Iteration 668, loss = 0.07546591\n",
      "Iteration 669, loss = 0.07487523\n",
      "Iteration 670, loss = 0.07431196\n",
      "Iteration 671, loss = 0.07414005\n",
      "Iteration 672, loss = 0.07427686\n",
      "Iteration 673, loss = 0.07432176\n",
      "Iteration 674, loss = 0.07428401\n",
      "Iteration 675, loss = 0.07393183\n",
      "Iteration 676, loss = 0.07352193\n",
      "Iteration 677, loss = 0.07327721\n",
      "Iteration 678, loss = 0.07330238\n",
      "Iteration 679, loss = 0.07336191\n",
      "Iteration 680, loss = 0.07329999\n",
      "Iteration 681, loss = 0.07306448\n",
      "Iteration 682, loss = 0.07279104\n",
      "Iteration 683, loss = 0.07249923\n",
      "Iteration 684, loss = 0.07232273\n",
      "Iteration 685, loss = 0.07227496\n",
      "Iteration 686, loss = 0.07225623\n",
      "Iteration 687, loss = 0.07213486\n",
      "Iteration 688, loss = 0.07191877\n",
      "Iteration 689, loss = 0.07167715\n",
      "Iteration 690, loss = 0.07151377\n",
      "Iteration 691, loss = 0.07143390\n",
      "Iteration 692, loss = 0.07136328\n",
      "Iteration 693, loss = 0.07123747\n",
      "Iteration 694, loss = 0.07106503\n",
      "Iteration 695, loss = 0.07092569\n",
      "Iteration 696, loss = 0.07076615\n",
      "Iteration 697, loss = 0.07064212\n",
      "Iteration 698, loss = 0.07049694\n",
      "Iteration 699, loss = 0.07033449\n",
      "Iteration 700, loss = 0.07020387\n",
      "Iteration 701, loss = 0.07008902\n",
      "Iteration 702, loss = 0.06999004\n",
      "Iteration 703, loss = 0.06989976\n",
      "Iteration 704, loss = 0.06977901\n",
      "Iteration 705, loss = 0.06962491\n",
      "Iteration 706, loss = 0.06945633\n",
      "Iteration 707, loss = 0.06929637\n",
      "Iteration 708, loss = 0.06915790\n",
      "Iteration 709, loss = 0.06903934\n",
      "Iteration 710, loss = 0.06893155\n",
      "Iteration 711, loss = 0.06882384\n",
      "Iteration 712, loss = 0.06870726\n",
      "Iteration 713, loss = 0.06857871\n",
      "Iteration 714, loss = 0.06844148\n",
      "Iteration 715, loss = 0.06830040\n",
      "Iteration 716, loss = 0.06816006\n",
      "Iteration 717, loss = 0.06802179\n",
      "Iteration 718, loss = 0.06788832\n",
      "Iteration 719, loss = 0.06775937\n",
      "Iteration 720, loss = 0.06763404\n",
      "Iteration 721, loss = 0.06751173\n",
      "Iteration 722, loss = 0.06739083\n",
      "Iteration 723, loss = 0.06727177\n",
      "Iteration 724, loss = 0.06715400\n",
      "Iteration 725, loss = 0.06703864\n",
      "Iteration 726, loss = 0.06692544\n",
      "Iteration 727, loss = 0.06681760\n",
      "Iteration 728, loss = 0.06669538\n",
      "Iteration 729, loss = 0.06704628\n",
      "Iteration 730, loss = 0.06898215\n",
      "Iteration 731, loss = 0.07479487\n",
      "Iteration 732, loss = 0.08413130\n",
      "Iteration 733, loss = 0.08192562\n",
      "Iteration 734, loss = 0.06891619\n",
      "Iteration 735, loss = 0.06910152\n",
      "Iteration 736, loss = 0.07691651\n",
      "Iteration 737, loss = 0.06905946\n",
      "Iteration 738, loss = 0.06737946\n",
      "Iteration 739, loss = 0.07310801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the MLP model\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8de9a7-862b-452a-b741-23e27095207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9e49d4-a656-4a9d-be32-d7848f8d7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        14\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.98      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy and classification report\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a70343-ed65-48ed-b3f1-d81a0e3b10c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.title('Loss Curve for MLP Classifier')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba222a-b26d-4d93-bb58-d8fa5be902c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
