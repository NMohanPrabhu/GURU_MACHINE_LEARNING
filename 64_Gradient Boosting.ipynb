{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b52117-5e90-4e05-9a31-a1b2b758c505",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Ensemble methods</font>\n",
    "<p> <font color=\"Yellow\" size=\"5\"><b>3_Gradient Boosting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08015eff-a8ca-48cd-b946-87bc5040ffc8",
   "metadata": {},
   "source": [
    "<p>Gradient Boosting is an ensemble learning method that builds strong predictive models by combining the predictions of several weak learners (typically decision trees) in a sequential manner. It works by training models in a stage-wise fashion, where each model is trained to correct the errors (residuals) of the previous one.\n",
    "\n",
    "Unlike AdaBoost, which adjusts the weights of incorrectly classified instances, Gradient Boosting minimizes the residual sum of squared errors in each iteration using gradient descent.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bacd1-6953-41e0-9543-31471860bdf4",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Key Concepts:</font>\n",
    "    <ol><li><font color=\"orange\">Boosting:</font> Gradient Boosting is a boosting method, which means it builds models sequentially and combines them to form a strong predictive model.</li>\n",
    "    <li><font color=\"orange\">Residuals:</font> In each step, the model focuses on the residual errors (the difference between the predicted and actual values from the previous model).</li>\n",
    "    <li><font color=\"orange\">Gradient Descent:</font> The algorithm minimizes the loss function using gradient descent, which adjusts the predictions of the model to reduce the error.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b31a948-73f5-4974-9609-88660c53df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9074\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93        19\n",
      "           1       0.90      0.90      0.90        21\n",
      "           2       1.00      0.79      0.88        14\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.92      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  0  0]\n",
      " [ 2 19  0]\n",
      " [ 1  2 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Create the Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4. Train the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Display the classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d407c3-79cb-431c-94e8-5c775d4752b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
