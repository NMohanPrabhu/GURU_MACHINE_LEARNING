{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba4432a-2732-412b-8b42-fea6147b7c04",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=6>8_Missing_data</font>\n",
    "<p><font color=\"yellow\" size=5>Imputation Techniques</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0ebc3-195e-47e4-9bf0-b5e2e2023180",
   "metadata": {},
   "source": [
    "In scikit-learn, imputation techniques are used to handle missing values in datasets. The library provides various tools to perform imputation efficiently, including strategies in the SimpleImputer and IterativeImputer classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4264ca-dae5-4597-8c8d-3790118e99db",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>1.Mean Imputation</font>\n",
    "\n",
    "Mean imputation replaces missing values in a dataset with the mean of the non-missing values for that feature (column). It is a simple and commonly used imputation technique for numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13751cfe-da55-4752-81e0-96411309378a",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Steps</font>\n",
    "<ol>\n",
    "   <li> Calculate the mean of the feature (excluding missing values).</li>\n",
    "     <li>Replace all missing values in the feature with the calculated mean.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28daa9a4-1d8f-418b-906b-95521b1850e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       5.0         9\n",
      "1       2.0       NaN        10\n",
      "2       NaN       NaN        11\n",
      "3       4.0       8.0        12\n",
      "\n",
      "DataFrame after Mean Imputation:\n",
      "   Feature1  Feature2  Feature3\n",
      "0  1.000000       5.0       9.0\n",
      "1  2.000000       6.5      10.0\n",
      "2  2.333333       6.5      11.0\n",
      "3  4.000000       8.0      12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1, 2, np.nan, 4],\n",
    "    'Feature2': [5, np.nan, np.nan, 8],\n",
    "    'Feature3': [9, 10, 11, 12]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize SimpleImputer with 'mean' strategy\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply imputation\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Mean Imputation:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff22e4b-9735-4525-86e4-5e96275b419c",
   "metadata": {},
   "source": [
    "<font color=\"orange\">For Feature1:\n",
    "<ol>\n",
    "    <li>Mean = (1+2+4)/3=2.3333</li>\n",
    "    <li>Missing value replaced with 2.3333</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881cf73-e792-4c11-9386-4c3d5ae85726",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>2.Median Imputation</font>\n",
    "\n",
    "Median imputation replaces missing values in a dataset with the median of the non-missing values for that feature (column). It is particularly useful for numerical data that is skewed or has outliers, as the median is more robust than the mean in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8125e427-87bd-49e0-8fd8-4c085e15687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       5.0         9\n",
      "1       2.0       NaN        10\n",
      "2       NaN       NaN        11\n",
      "3     100.0       8.0        12\n",
      "\n",
      "DataFrame after Median Imputation:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       5.0       9.0\n",
      "1       2.0       6.5      10.0\n",
      "2       2.0       6.5      11.0\n",
      "3     100.0       8.0      12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1, 2, np.nan, 100],  # Skewed due to an outlier (100)\n",
    "    'Feature2': [5, np.nan, np.nan, 8],\n",
    "    'Feature3': [9, 10, 11, 12]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize SimpleImputer with 'median' strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Apply imputation\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Median Imputation:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160c08b-443e-4969-bdcc-ef50427bd7d7",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Explanation</font>\n",
    "  <li><b>For Feature1:</b></li>\n",
    "       <ol><li> Median = 2.0 (sorted values: 1,2,100).</li>\n",
    "       <li>Missing value replaced with 2.0</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02f4d9-e523-4e4a-99dd-cfe579f400ed",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>3.Most Frequent (Mode) Imputation</font>\n",
    "\n",
    "Most frequent imputation replaces missing values with the most frequently occurring value (mode) in the column. This technique is particularly suitable for categorical data, but it can also be applied to numerical data with repeating values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845fa94a-c998-4729-92f0-e90026b8b83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Feature1  Feature2\n",
      "0      red       1.0\n",
      "1     blue       2.0\n",
      "2      NaN       2.0\n",
      "3     blue       NaN\n",
      "4      red       2.0\n",
      "5     blue       2.0\n",
      "\n",
      "DataFrame after Most Frequent Imputation:\n",
      "  Feature1 Feature2\n",
      "0      red      1.0\n",
      "1     blue      2.0\n",
      "2     blue      2.0\n",
      "3     blue      2.0\n",
      "4      red      2.0\n",
      "5     blue      2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': ['red', 'blue', np.nan, 'blue', 'red', 'blue'],  # Categorical\n",
    "    'Feature2': [1, 2, 2, np.nan, 2, 2],  # Numerical\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize SimpleImputer with 'most_frequent' strategy\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply imputation\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Most Frequent Imputation:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19705b-1b07-45dc-90a9-8ddbf50a86aa",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Explanation\n",
    "<ol>\n",
    "     <li><font color=\"sky blue\">For Feature1 (categorical):</font></li>\n",
    "       <ol><li> Most frequent value = blue (appears 3 times).</li>\n",
    "        <li>Missing value replaced with blue.</li></ol>\n",
    "     <li><font color=\"sky blue\">For Feature2 (numerical):</li>\n",
    "       <ol> <li>Most frequent value = 2.0 (appears 4 times).</li>\n",
    "       <li> Missing value replaced with 2.0</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e932861-8de6-4274-ace0-8fe1fd3f2ee7",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>4.Constant Value Imputation</font>\n",
    "\n",
    "Constant value imputation involves replacing missing values with a user-specified constant. This method can be used for both categorical and numerical data. It is particularly useful when you want to assign a meaningful default value to missing entries, such as \"Unknown\" for categorical data or 0 for numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d4cc40-a1b3-4759-9938-9179263cc97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Feature1  Feature2\n",
      "0    apple      10.0\n",
      "1   banana      20.0\n",
      "2      NaN       NaN\n",
      "3   orange      40.0\n",
      "\n",
      "DataFrame after Constant Value Imputation:\n",
      "  Feature1  Feature2\n",
      "0    apple      10.0\n",
      "1   banana      20.0\n",
      "2  missing       0.0\n",
      "3   orange      40.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': ['apple', 'banana', np.nan, 'orange'],  # Categorical\n",
    "    'Feature2': [10, 20, np.nan, 40],  # Numerical\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize SimpleImputer with 'constant' strategy\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='missing')  # For categorical\n",
    "df_categorical_imputed = pd.DataFrame(imputer.fit_transform(df[['Feature1']]), columns=['Feature1'])\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)  # For numerical\n",
    "df_numerical_imputed = pd.DataFrame(imputer.fit_transform(df[['Feature2']]), columns=['Feature2'])\n",
    "\n",
    "# Combine the results\n",
    "df_imputed = pd.concat([df_categorical_imputed, df_numerical_imputed], axis=1)\n",
    "\n",
    "print(\"\\nDataFrame after Constant Value Imputation:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2f9cb-d9fd-4f1e-acf1-76f225c4022a",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>5.Iterative Imputer</font>\n",
    "\n",
    "The IterativeImputer in scikit-learn is a multivariate imputation method that models each feature with missing values as a function of the other features. It iteratively predicts missing values by fitting a regression model for each feature with missing values, based on the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7f1cc-d61d-42c2-895f-ed927b287625",
   "metadata": {},
   "source": [
    "**How It Works**\n",
    "\n",
    "Initialize missing values using a simple method (e.g., mean, median, or other).\n",
    "\n",
    "<ol>\n",
    "<font color = \"yellow\">For each feature with missing values:</font>\n",
    "<li>Treat it as the target variable.</li>\n",
    "<li>Use the other features as predictors in a regression model to predict the missing values.</li>\n",
    "<li>Repeat this process for all features with missing values iteratively.</li>\n",
    "<li>Continue iterations until convergence or a specified maximum number of iterations.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51bf11-6ad7-49d4-a58f-13181bd1ba4f",
   "metadata": {},
   "source": [
    "**Key Parameters**\n",
    "\n",
    "<ol>\n",
    "<font color><li>estimator:</font>\n",
    "    The model used to predict missing values (default: BayesianRidge).</li>\n",
    "<font color><li>max_iter:</font> \n",
    "    Maximum number of iterations to run (default: 10).</li>\n",
    "<font color><li>random_state:</font> \n",
    "    Ensures reproducibility of results.</li>\n",
    "<font color><li>tol:</font> \n",
    "    Convergence threshold; stops iterations if changes are smaller than this value.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd66519-903b-47a8-8501-b1d8975fb0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       NaN       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       NaN       6.0       9.0\n",
      "3       4.0       8.0       NaN\n",
      "\n",
      "DataFrame after Iterative Imputation:\n",
      "   Feature1  Feature2  Feature3\n",
      "0  1.000000  0.746042  7.000000\n",
      "1  2.000000  3.000000  8.000000\n",
      "2  3.162385  6.000000  9.000000\n",
      "3  4.000000  8.000000  9.792037\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, np.nan, 4.0],\n",
    "    'Feature2': [np.nan, 3.0, 6.0, 8.0],\n",
    "    'Feature3': [7.0, 8.0, 9.0, np.nan],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize IterativeImputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Apply the IterativeImputer\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Iterative Imputation:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ec12c-3c24-4234-ad5d-bf17d4d5f316",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "\n",
    "The imputer uses all available data to iteratively estimate missing values.\n",
    "<ol>\n",
    "<font color = \"yellow\">For example:</font>\n",
    "<li>Missing value in Feature1 was estimated using relationships with Feature2 and Feature3.</li>\n",
    "<li>Missing value in Feature3 was estimated using Feature1 and Feature2.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba626a92-c88e-4b48-813f-63a684cd4822",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"5\">5.1_IterativeImputer with Random Forest Regressor</font>\n",
    "\n",
    "The IterativeImputer can be customized to use a Random Forest Regressor as the estimator for predicting missing values. This can improve imputation accuracy when the data has non-linear relationships or complex patterns, as Random Forest is a robust and flexible non-linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d876716a-1ed2-4d83-8ed2-b909792c2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       NaN       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       NaN       6.0       9.0\n",
      "3       4.0       8.0       NaN\n",
      "\n",
      "DataFrame after Iterative Imputation with Random Forest Regressor:\n",
      "   Feature1  Feature2  Feature3\n",
      "0      1.00       4.4      7.00\n",
      "1      2.00       3.0      8.00\n",
      "2      2.62       6.0      9.00\n",
      "3      4.00       8.0      8.53\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, np.nan, 4.0],\n",
    "    'Feature2': [np.nan, 3.0, 6.0, 8.0],\n",
    "    'Feature3': [7.0, 8.0, 9.0, np.nan],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize IterativeImputer with RandomForestRegressor\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=100, random_state=0), max_iter=10, random_state=0)\n",
    "\n",
    "# Apply the imputer\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Iterative Imputation with Random Forest Regressor:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03f85c-e72e-4adc-a635-a1b0e61e9491",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"5\">5.2_IterativeImputer with Decision Tree Regressor</font>\n",
    "\n",
    "The IterativeImputer can be paired with a Decision Tree Regressor to impute missing values in a dataset. A Decision Tree Regressor is particularly useful when there are complex relationships in the data, and the dataset has non-linear patterns or interactions. It also handles categorical splits naturally when features are encoded numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640596ff-a125-4479-9b6c-68150de22d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       NaN       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       NaN       6.0       9.0\n",
      "3       4.0       8.0       NaN\n",
      "\n",
      "DataFrame after Iterative Imputation with Decision Tree Regressor:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       6.0       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       1.0       6.0       9.0\n",
      "3       4.0       8.0       8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer  # Required to enable IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, np.nan, 4.0],\n",
    "    'Feature2': [np.nan, 3.0, 6.0, 8.0],\n",
    "    'Feature3': [7.0, 8.0, 9.0, np.nan],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize IterativeImputer with DecisionTreeRegressor\n",
    "imputer = IterativeImputer(\n",
    "    estimator=DecisionTreeRegressor(max_depth=5, random_state=0), \n",
    "    max_iter=10, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Apply the imputer\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Iterative Imputation with Decision Tree Regressor:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba12336-55b6-4ce8-b5ca-51c694ca812d",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"4\">Customizing the Decision Tree</font>\n",
    "\n",
    "You can fine-tune the Decision Tree Regressor by specifying parameters such as max_depth, min_samples_split, and min_samples_leaf. For example:\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    estimator=DecisionTreeRegressor(max_depth=3, min_samples_split=5, random_state=42),\n",
    "    max_iter=15,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e5019-3d75-42bd-a555-4bc341e548af",
   "metadata": {},
   "source": [
    "**Best Practices**\n",
    "\n",
    "<ol>\n",
    "<font color = \"yellow\"><li>Preprocessing:</font>\n",
    "        Encode categorical variables (e.g., using one-hot or label encoding).</li>\n",
    "<font color = \"yellow\"><li>Regularization:</font>\n",
    "        Use hyperparameters like max_depth and min_samples_leaf to prevent overfitting.</li>\n",
    "<font color = \"yellow\"><li>Validation:</font>\n",
    "        Check the imputation quality using cross-validation or by examining downstream model performance.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abe360-a8c0-4c53-9a6d-28084168ac3e",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"5\">6_KNNImputer</font>\n",
    "\n",
    "The KNNImputer replaces missing values in a dataset by using the values of the k-nearest neighbors of each data point with a missing value. The key idea is that data points that are similar (i.e., near each other in feature space) are likely to have similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9da8eaf-9229-43c0-a3cc-ece0121e05ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       NaN       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       NaN       6.0       9.0\n",
      "3       4.0       8.0       NaN\n",
      "\n",
      "DataFrame after KNN Imputation:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       4.5       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       2.5       6.0       9.0\n",
      "3       4.0       8.0       8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, np.nan, 4.0],\n",
    "    'Feature2': [np.nan, 3.0, 6.0, 8.0],\n",
    "    'Feature3': [7.0, 8.0, 9.0, np.nan],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "\n",
    "# Apply the KNNImputer\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after KNN Imputation:\")\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f3d80-78b1-4716-a45d-308f39b2c0da",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"5\">7_MissingIndicator in Scikit-learn</font>\n",
    "\n",
    "The MissingIndicator class in scikit-learn is a transformer used to create an indicator matrix indicating where missing values (NaN) are located in the original dataset. It is useful when you want to retain the information about the presence of missing values as part of the features for a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01d4f20-9bfb-4921-bd4b-f6df4997b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Feature3\n",
      "0       1.0       NaN       7.0\n",
      "1       2.0       3.0       8.0\n",
      "2       NaN       6.0       9.0\n",
      "3       4.0       8.0       NaN\n",
      "\n",
      "Missing Value Indicator Matrix:\n",
      "   Feature1_missing  Feature2_missing  Feature3_missing\n",
      "0             False              True             False\n",
      "1             False             False             False\n",
      "2              True             False             False\n",
      "3             False             False              True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, np.nan, 4.0],\n",
    "    'Feature2': [np.nan, 3.0, 6.0, 8.0],\n",
    "    'Feature3': [7.0, 8.0, 9.0, np.nan]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize the MissingIndicator\n",
    "indicator = MissingIndicator()\n",
    "\n",
    "# Fit and transform the data\n",
    "indicator_matrix = indicator.fit_transform(df)\n",
    "\n",
    "# Create a DataFrame for the indicator matrix\n",
    "df_indicator = pd.DataFrame(indicator_matrix, columns=[f\"{col}_missing\" for col in df.columns])\n",
    "\n",
    "print(\"\\nMissing Value Indicator Matrix:\")\n",
    "print(df_indicator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b947a1c-f18b-4ec4-b755-5c6f2cfe55b1",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"5\">8_Using Pipelines for Imputation in Scikit-learn</font>\n",
    "\n",
    "In scikit-learn, Pipelines provide a convenient way to organize and streamline the process of transforming data and applying models. A pipeline allows you to chain multiple steps together, such as data preprocessing, imputation, feature scaling, and model training.\n",
    "\n",
    "By combining imputation into a pipeline, you can ensure that missing data is handled consistently before training your model. This is particularly useful for creating robust workflows in machine learning and ensuring the steps are applied in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d591d036-1d2d-4556-ac30-16a6d61ea88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example Dataset with Missing Values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, np.nan, 4.0],\n",
    "    'Feature2': [np.nan, 3.0, 6.0, 8.0],\n",
    "    'Feature3': [7.0, 8.0, 9.0, np.nan],\n",
    "    'Target': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define a pipeline with imputation and model\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputation step\n",
    "    ('classifier', RandomForestClassifier())     # Model step\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a26f0-a791-400a-85a2-a975f5ba8ac2",
   "metadata": {},
   "source": [
    "<font color = \"yellow\" size=\"5\">9_ColumnTransformer</font>\n",
    "<ol>\n",
    "It allows you to apply different preprocessing techniques to different subsets of columns in your dataset. It’s typically used in a pipeline to apply transformations like imputation, scaling, or encoding to the relevant features.</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236a7df-c465-458b-a8a7-c0b42ce76ee5",
   "metadata": {},
   "source": [
    "We'll use a small dataset with both numerical and categorical columns. We'll:\n",
    "<ol>\n",
    "    <li>Impute missing values in numerical columns.</li>\n",
    "    <li>Scale the numerical columns.</li>\n",
    "    <li>One-hot encode the categorical columns.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a92fa46-c8ac-4da4-a555-9ab777d66dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Age    Salary  Gender_Female  Gender_Male  City_Chicago  \\\n",
      "0 -1.414214 -1.543033            0.0          1.0           0.0   \n",
      "1  0.000000  0.308607            1.0          0.0           0.0   \n",
      "2  0.000000  1.234427            1.0          0.0           1.0   \n",
      "3  1.414214  0.000000            0.0          1.0           0.0   \n",
      "\n",
      "   City_Los Angeles  City_New York  \n",
      "0               0.0            1.0  \n",
      "1               1.0            0.0  \n",
      "2               0.0            0.0  \n",
      "3               0.0            1.0  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Example dataset with missing values\n",
    "data = {\n",
    "    'Age': [25, 30, np.nan, 35],\n",
    "    'Salary': [50000, 60000, 65000, np.nan],\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male'],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the columns for transformation\n",
    "numerical_features = ['Age', 'Salary']\n",
    "categorical_features = ['Gender', 'City']\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values\n",
    "            ('scaler', StandardScaler())                 # Scale numerical columns\n",
    "        ]), numerical_features),                         # Apply to numerical columns\n",
    "        \n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values for categorical\n",
    "            ('onehot', OneHotEncoder())                            # One-hot encode categorical columns\n",
    "        ]), categorical_features)                             # Apply to categorical columns\n",
    "    ],\n",
    "    remainder='passthrough'  # Columns not specified will be passed through unchanged\n",
    ")\n",
    "\n",
    "# Apply the transformations\n",
    "X_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Dynamically generate column names for the transformed data\n",
    "# Get the column names for OneHotEncoding\n",
    "ohe_columns = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine column names for numerical features and the one-hot encoded columns\n",
    "columns = numerical_features + ohe_columns.tolist()\n",
    "\n",
    "# Convert the result into a DataFrame for better visualization\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=columns)\n",
    "\n",
    "print(X_transformed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d378e2e5-442a-4462-ab93-856ce1c32466",
   "metadata": {},
   "source": [
    "**Explanation of Key Parts:**\n",
    "\n",
    "<ol>\n",
    "<font color = \"yellow\"><li>Imputation:</font>\n",
    "        We use SimpleImputer(strategy='mean') for numerical columns (Age, Salary) to replace missing values with the column mean.\n",
    "        For categorical columns (Gender, City), we use SimpleImputer(strategy='most_frequent') to impute missing values with the most frequent category.</li>\n",
    "\n",
    "<font color = \"yellow\"><li>Scaling:</font>\n",
    "        We apply StandardScaler to scale the numerical columns to have zero mean and unit variance.</li>\n",
    "\n",
    "<font color = \"yellow\"><li>One-Hot Encoding:</font>\n",
    "        OneHotEncoder is used for the categorical columns (Gender, City) to create binary columns representing each category.</li>\n",
    "\n",
    "<font color = \"yellow\"><li>Dynamic Column Names:</font>\n",
    "        For the categorical columns, we retrieve the column names generated by OneHotEncoder using get_feature_names_out().\n",
    "        The final column names are combined from the numerical columns and the one-hot encoded categorical columns.</li>\n",
    "\n",
    "<font color = \"yellow\"><li>remainder='passthrough':</font>\n",
    "        This ensures that any columns not specified in the ColumnTransformer are passed through unchanged. In this case, all columns are processed, so it doesn’t affect the result.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2bd11-c5e1-44bb-9fd9-5b003a326321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
