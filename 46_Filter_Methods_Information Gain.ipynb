{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83057aaf-41c5-4f5d-a4e6-0606ad2ddd55",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Filter Methods</font>\n",
    "<p><font color=\"Yellow\" size=\"4\">6_Information_Gain</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0cb2ff-6ad8-4006-a1ab-84bbdf6d7e17",
   "metadata": {},
   "source": [
    "*Information Gain (IG) is a key concept in machine learning and is often used in decision trees (like in the ID3 algorithm) to determine the best feature to split the data on. Information Gain measures the reduction in uncertainty or entropy after a dataset is split based on a particular feature.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9301718c-5b7f-4769-a87b-30c7a170bf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for 'Outlook': 0.24674981977443933\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Create a simple dataset\n",
    "data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(data):\n",
    "    value_counts = data.value_counts(normalize=True)\n",
    "    return -np.sum(value_counts * np.log2(value_counts))\n",
    "\n",
    "# Function to calculate information gain\n",
    "def information_gain(df, feature, target):\n",
    "    # Calculate the entropy of the whole dataset\n",
    "    original_entropy = entropy(df[target])\n",
    "    \n",
    "    # Calculate the weighted average entropy after the split based on the feature\n",
    "    feature_values = df[feature].unique()\n",
    "    weighted_entropy = 0\n",
    "    for value in feature_values:\n",
    "        subset = df[df[feature] == value]\n",
    "        weighted_entropy += (len(subset) / len(df)) * entropy(subset[target])\n",
    "    \n",
    "    # Information Gain is the reduction in entropy\n",
    "    return original_entropy - weighted_entropy\n",
    "\n",
    "# Calculate Information Gain for the 'Outlook' feature\n",
    "ig_outlook = information_gain(df, 'Outlook', 'PlayTennis')\n",
    "print(f\"Information Gain for 'Outlook': {ig_outlook}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fd2c0-161d-4869-833f-ed76773b8717",
   "metadata": {},
   "source": [
    "<b><font color=\"orange\">ALTERNATE METHOD:</font></b>\n",
    "The scikit-learn library provides several methods for feature selection, although it does not directly provide a function specifically for Information Gain. However, you can use mutual information (which is related to Information Gain) for feature selection. For classification tasks, mutual_info_classif can be used to compute the mutual information between each feature and the target variable, which can be considered as a measure of Information Gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3d44cf-6001-456f-b29a-e8524a3d1616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Mutual Information\n",
      "2  petal length (cm)            0.999752\n",
      "3   petal width (cm)            0.978626\n",
      "0  sepal length (cm)            0.524433\n",
      "1   sepal width (cm)            0.261887\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target variable\n",
    "\n",
    "# Calculate mutual information between each feature and the target\n",
    "mi = mutual_info_classif(X, y)\n",
    "\n",
    "# Display the results\n",
    "feature_names = iris.feature_names\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Mutual Information': mi\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "print(mi_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fac93e-a60e-4a81-981d-04f6afcfeb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
