{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98709dac-69da-4dc1-a4f7-1f9d5967b277",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\"><b>Ensemble Methods</font>\n",
    "<p><font color=\"Yellow\" size=\"5\"><b>3_RUSBoostClassifier</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca339b8-93fa-4fd8-abd1-7ce2d4c1d277",
   "metadata": {},
   "source": [
    "The RUSBoostClassifier is an ensemble method available in the imbalanced-learn library. It combines Random Under-Sampling (RUS) with the Boosting algorithm to handle imbalanced datasets. It is based on the principle of adaptive boosting (AdaBoost), where weak learners are trained iteratively, but the training data is balanced in each iteration by under-sampling the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3178c-9832-47b7-9a21-481393bddf1b",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Key Characteristics of RUSBoostClassifier:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Random Under-Sampling:</font> At each boosting iteration, the majority class is randomly under-sampled to match the size of the minority class.</li>\n",
    "    <li><font color=\"orange\">Boosting:</font> Boosting assigns weights to misclassified samples to focus on hard-to-classify instances in subsequent iterations.</li>\n",
    "    <li><font color=\"orange\">Base Estimator:</font> You can specify the weak learner (default is a decision tree).</li>\n",
    "    <li><font color=\"orange\">Handles Class Imbalance:</font> Works well for highly imbalanced datasets by combining under-sampling with boosting.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f53d6-e56c-4771-ae8a-b30c66220786",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Parameters of RUSBoostClassifier</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">base_estimator:</font> Specify the weak learner (default is DecisionTreeClassifier(max_depth=1)).</li>\n",
    "   <li><font color=\"orange\"> n_estimators:</font> Number of boosting iterations (default: 50).</li>\n",
    "    <li><font color=\"orange\">sampling_strategy:</font> Proportion of minority to majority class after under-sampling (default: auto).</li>\n",
    "    <li><font color=\"orange\">random_state:</font> Seed for reproducibility.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1add9236-08dd-4ea0-8cbc-8387dfc53a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before resampling: Counter({0: 898, 1: 102})\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96       275\n",
      "           1       0.59      0.80      0.68        25\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.78      0.87      0.82       300\n",
      "weighted avg       0.95      0.94      0.94       300\n",
      "\n",
      "Class distribution after resampling in predictions: Counter({0: 266, 1: 34})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
    "                            n_redundant=10, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Step 2: Check class distribution\n",
    "print(\"Class distribution before resampling:\", Counter(y))\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Initialize the RUSBoostClassifier\n",
    "rusboost = RUSBoostClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "# Step 5: Train the RUSBoostClassifier\n",
    "rusboost.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = rusboost.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Check class distribution after resampling\n",
    "print(\"Class distribution after resampling in predictions:\", Counter(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934671b3-6add-4d89-860f-413b9a28deb5",
   "metadata": {},
   "source": [
    "<b><font color=\"sky blue\">When to Use RUSBoostClassifier?</font></b>\n",
    "<ol>\n",
    "   <li>When you have a highly imbalanced dataset, and simple under-sampling methods lead to loss of important data.</li> \n",
    "    <li> When you want to combine the power of boosting (focus on hard examples) with balancing techniques.</li> </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f3368-5575-4416-8185-59a9e7fb5e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
