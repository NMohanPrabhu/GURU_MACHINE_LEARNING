{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ed5d6e-bb7f-4669-9c7d-d0d4200a9cdb",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">NearMiss</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d226aa-b578-4faf-8779-2e8ab1bf6a5e",
   "metadata": {},
   "source": [
    "The NearMiss technique is not a classifier itself, but rather a data preprocessing method for handling class imbalance by performing under-sampling on the majority class. It is available in the imbalanced-learn library under the under_sampling module. The technique focuses on retaining samples from the majority class that are closest to the minority class, ensuring a more informative subset of data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129246c-8838-4dd6-80a8-e67f0370bfdc",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Key Features of NearMiss:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Under-Sampling Method:</font> Reduces the majority class samples to balance the dataset.</li>\n",
    "    <li><font color=\"saffron\">Strategies:</font>\n",
    "        <ol><li>NearMiss-1: Selects majority samples with the smallest average distance to the k-nearest neighbors of the minority class.</li>\n",
    "        <li>NearMiss-2: Selects majority samples with the smallest average distance to the farthest k-nearest neighbors of the minority class.</li>\n",
    "        <li>NearMiss-3: Retains majority samples closest to each minority sample until the desired balance is achieved.</li></ol></li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7951e145-9a9a-49e3-8981-d0478b706452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before NearMiss: Counter({0: 898, 1: 102})\n",
      "Class distribution after NearMiss: Counter({0: 102, 1: 102})\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        32\n",
      "           1       0.87      0.87      0.87        30\n",
      "\n",
      "    accuracy                           0.87        62\n",
      "   macro avg       0.87      0.87      0.87        62\n",
      "weighted avg       0.87      0.87      0.87        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2,\n",
    "                            n_redundant=10, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Step 2: Check class distribution\n",
    "print(\"Class distribution before NearMiss:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply NearMiss for under-sampling\n",
    "nm = NearMiss(version=1)  # Specify the NearMiss version (1, 2, or 3)\n",
    "X_res, y_res = nm.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the new class distribution\n",
    "print(\"Class distribution after NearMiss:\", Counter(y_res))\n",
    "\n",
    "# Step 5: Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier on the balanced dataset\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the classifier\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137c186-3a5c-4ddb-ac7e-1c09d7f5cca0",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Parameters of NearMiss</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">version:</font> The version of the NearMiss algorithm (1, 2, or 3).\n",
    "    <li><font color=\"orange\">sampling_strategy:</font> The ratio of minority to majority class after resampling.\n",
    "    <li><font color=\"orange\">n_neighbors:</font> Number of nearest neighbors to consider for majority class selection (default: 3).\n",
    "    <li><font color=\"orange\">n_neighbors_ver3:</font> Used only for version=3, specifies the number of neighbors to use for the minority class.\n",
    "    <li><font color=\"orange\">random_state:</font> Seed for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd85c-eeb4-4aa0-b434-605aecfb2338",
   "metadata": {},
   "source": [
    "<b><font color=\"sky blue\">When to Use NearMiss?</font></b>\n",
    "<ol>\n",
    "   <li>When you want to balance an imbalanced dataset by selectively under-sampling the majority class.</li>\n",
    "    <li>When the dataset size is large, and removing redundant majority samples improves computational efficiency.</li>\n",
    "    <li>When you want to retain majority class samples that are informative and close to the minority class.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b50ce6-cbb4-4280-b736-c59acc90820c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
