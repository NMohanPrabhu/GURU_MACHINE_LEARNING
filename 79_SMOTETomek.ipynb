{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9f90f8-d788-44f3-aef0-d7eca5756600",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Hybrid Model</font>\n",
    "<p><font color=\"Yellow\" size=\"5\"><b>2_SMOTETomek (SMOTE + Tomek Links)</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386739a8-15c2-400d-9d9f-b52df56f381b",
   "metadata": {},
   "source": [
    "<b><font size=4>SMOTETomek</font></b>\n",
    "\n",
    "<font color=\"pink\" size=4>SMOTETomek is another hybrid technique designed to handle class imbalance in datasets. It combines two methods:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">SMOTE (Synthetic Minority Over-sampling Technique):</font> This technique generates synthetic samples for the minority class by interpolating between existing minority class samples.</li>\n",
    "    <li><font color=\"orange\">Tomek Links:</font> Tomek Links is an under-sampling technique that removes pairs of samples from different classes that are close to each other (i.e., they are neighbors). The idea is to eliminate borderline examples or ambiguous samples that lie close to the decision boundary between classes, which could confuse a classifier.</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0e91d-a9a6-47f6-a57f-45b97d697488",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>How SMOTETomek Works:\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Generate Synthetic Minority Samples (SMOTE):</font> \n",
    "        The first step in SMOTETomek is to apply SMOTE to generate synthetic samples for the minority class. This increases the number of minority class instances by interpolating between existing minority class samples.</li>\n",
    "    <li><font color=\"orange\">Remove Borderline Samples Using Tomek Links:</font> \n",
    "        After applying SMOTE, Tomek Links is applied to remove pairs of samples that are neighbors but belong to different classes. These pairs are likely to be borderline samples that are close to the decision boundary and could introduce noise or ambiguity into the classifier.</li>\n",
    "    <li><font color=\"orange\">Balanced Dataset:</font> \n",
    "        The result is a dataset that is both balanced and cleaned of noisy or borderline samples, leading to potentially better generalization by the model.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a446680-5e52-4dc1-a66b-25ade271f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTETomek: Counter({0: 898, 1: 102})\n",
      "Class distribution after SMOTETomek: Counter({0: 896, 1: 896})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       264\n",
      "           1       0.96      0.96      0.96       274\n",
      "\n",
      "    accuracy                           0.96       538\n",
      "   macro avg       0.96      0.96      0.96       538\n",
      "weighted avg       0.96      0.96      0.96       538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying SMOTETomek\n",
    "print(\"Class distribution before SMOTETomek:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply SMOTETomek to balance the dataset\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the class distribution after applying SMOTETomek\n",
    "print(\"Class distribution after SMOTETomek:\", Counter(y_resampled))\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (RandomForest) on the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092525e-f93f-412e-9990-3ad317ac2d74",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Advantages of SMOTETomek:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Improved Decision Boundaries:</font> The Tomek Links step helps to clean the dataset by removing borderline or noisy instances, resulting in better decision boundaries between classes.\n",
    "    <li><font color=\"orange\">Hybrid Approach:</font> SMOTETomek combines both over-sampling and under-sampling techniques, making it a more sophisticated solution than applying either technique alone.</li>\n",
    "    <li><font color=\"orange\">Reduces Overfitting:</font> By cleaning the dataset of noisy instances, SMOTETomek helps the classifier generalize better, reducing the chances of overfitting to the minority class.</li></ol>\n",
    "\n",
    "<font color=\"pink\" size=4>Drawbacks of SMOTETomek:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Computational Complexity:</font> Since SMOTETomek involves both over-sampling and under-sampling steps, it can be computationally expensive, especially for large datasets.</li>\n",
    "    <li><font color=\"orange\">Loss of Information:</font> The Tomek Links step may lead to the removal of useful majority class samples, which could impact model performance in certain cases.</li>\n",
    "    <li><font color=\"orange\">Not Always Effective:</font> If the dataset doesn't have significant noise or borderline instances, the effectiveness of SMOTETomek may be limited.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be49d2-c2d4-4a8e-8519-faa0ab3d60f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
