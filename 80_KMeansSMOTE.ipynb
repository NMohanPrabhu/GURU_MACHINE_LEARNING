{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aac2ba-dc1d-4e57-96c9-5dbc29899ef7",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\"><b>Nearest Neighbor Methods</font>\n",
    "<p><font color=\"Yellow\" size=\"5\"><b>1_KMeansSMOTE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb5f444-6e97-4250-bb9b-0d2d8c4f2c3b",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>KMeansSMOTE (K-Means Synthetic Minority Over-sampling Technique)</font>\n",
    "\n",
    "KMeansSMOTE is an advanced technique used to handle class imbalance in datasets, particularly when data is clustered or has a complex structure. It combines K-Means clustering and SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for the minority class in an optimal way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6357a-f50f-45e6-bcb8-42f69bfa41c9",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>How KMeansSMOTE Works:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">K-Means Clustering:</font>\n",
    "        The dataset is clustered into k clusters using the K-Means algorithm. The goal is to find clusters of minority class samples that can be used to generate synthetic data points.</li>\n",
    "    <li><font color=\"orange\">SMOTE:</font>\n",
    "        After the clusters are formed, the SMOTE algorithm is used to generate synthetic data points within each cluster. These synthetic points are based on the nearest neighbors of each minority sample in the cluster.</li>\n",
    "    <li><font color=\"orange\">Balanced Dataset:</font>\n",
    "        The result is a balanced dataset where the minority class has been augmented using synthetic data points generated in a structured and optimized manner based on the clustering.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e87d153-477e-4713-95d5-2c8b8cf10034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a8d61d7-ad36-43b2-856a-8050ef47c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import KMeansSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d1d24aa-3152-4417-af18-fbc48269e76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before KMeansSMOTE: Counter({0: 898, 1: 102})\n",
      "Class distribution after KMeansSMOTE: Counter({0: 898, 1: 898})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\root\\anaconda3\\envs\\mydl\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       265\n",
      "           1       0.99      0.97      0.98       274\n",
      "\n",
      "    accuracy                           0.98       539\n",
      "   macro avg       0.98      0.98      0.98       539\n",
      "weighted avg       0.98      0.98      0.98       539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from imblearn.combine import KMeansSMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying KMeansSMOTE\n",
    "print(\"Class distribution before KMeansSMOTE:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply KMeansSMOTE to balance the dataset\n",
    "kmeans_smote = KMeansSMOTE(random_state=42)\n",
    "X_resampled, y_resampled = kmeans_smote.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the class distribution after applying KMeansSMOTE\n",
    "print(\"Class distribution after KMeansSMOTE:\", Counter(y_resampled))\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (RandomForest) on the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdcd9f-1e5d-4610-9af0-5773941b0e41",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Advantages of KMeansSMOTE:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Optimal Synthetic Data Generation:</font> By clustering minority class samples and generating synthetic data points within each cluster, KMeansSMOTE can produce more meaningful synthetic data, leading to better generalization.</li>\n",
    "    <li><font color=\"orange\">Handling Complex Data Structures:</font> The clustering step allows handling datasets with non-linear distributions and complex relationships.</li>\n",
    "    <li><font color=\"orange\">Reduces Noise:</font> By generating data points only from similar samples within clusters, the technique reduces the risk of introducing noise.</li></ol>\n",
    "\n",
    "<font color=\"pink\" size=4>Drawbacks of KMeansSMOTE:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Computational Complexity:</font> The clustering step involved in K-MeansSMOTE can be computationally expensive, especially for large datasets.</li>\n",
    "    <li><font color=\"orange\">Tuning Hyperparameters:</font> The performance of KMeansSMOTE depends on the choice of the number of clusters (k), which needs careful tuning.</li>\n",
    "    <li><font color=\"orange\">Limited Availability:</font> The method is less commonly implemented and may not be readily available in all libraries.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a2756-fcf1-438c-9f11-71084d68eb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
