{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bac40f9-f3e6-4d4c-b555-eec189482361",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Undersampling Methods</font>\n",
    "<p><font color=\"yellow\" size=\"5\"><b>1_RandomUnderSampler </b></font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa90f46-266f-48d9-8a92-fd43f6ef724c",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4><b>RandomUnderSampler</b>\n",
    "\n",
    "RandomUnderSampler is an oversampling technique used to handle class imbalance in datasets. Unlike techniques like SMOTE or RandomOverSampler, which generate synthetic samples for the minority class, RandomUnderSampler addresses the class imbalance by randomly removing samples from the majority class.\n",
    "\n",
    "The idea behind RandomUnderSampler is to reduce the number of majority class samples so that the class distribution becomes more balanced. While this approach is simple and effective, it may result in the loss of potentially valuable data, especially if the majority class contains important samples that could improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f18f3f-d3e3-437a-acec-37049cb93a4d",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>How RandomUnderSampler Works:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Identify Majority Class:</font> The first step is to identify the class that is overrepresented (majority class).<?\n",
    "    <li><font color=\"orange\">Random Sampling:</font> A random subset of samples from the majority class is selected, and these samples are removed from the dataset.\n",
    "    <li><font color=\"orange\">Resampling:</font> After removing samples, the dataset will be balanced with respect to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8e5f1-3c84-4bfd-9c02-a075edad84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying RandomUnderSampler\n",
    "print(\"Class distribution before RandomUnderSampler:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply RandomUnderSampler to balance the class distribution\n",
    "random_under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = random_under_sampler.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the class distribution after applying RandomUnderSampler\n",
    "print(\"Class distribution after RandomUnderSampler:\", Counter(y_resampled))\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (RandomForest) on the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f294646-eb25-4d98-9765-e5d222a4431c",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Advantages of RandomUnderSampler:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Simple and Easy to Implement:</font> It is easy to apply and does not require complex computations.</li>\n",
    "    <li><font color=\"orange\">Reduces Overfitting:</font> By reducing the number of majority class samples, it may help reduce overfitting in some cases.</li>\n",
    "    <li><font color=\"orange\">Improved Computational Efficiency:</font> Reducing the number of majority class samples can speed up the training process.</li></ol>\n",
    "\n",
    "<font color=\"pink\" size=4>Drawbacks of RandomUnderSampler:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Loss of Valuable Data:</font> By removing samples from the majority class, there is a risk of losing valuable information, especially if the majority class is large and diverse.</li>\n",
    "    <li><font color=\"orange\">Risk of Underfitting:</font> If too many majority class samples are removed, the model may underfit, as it will not have enough information to learn from.</li>\n",
    "    <li><font color=\"orange\">Bias Toward Minority Class:</font> Over-sampling the minority class or under-sampling the majority class may lead to a model that has biased performance or poor generalization.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f4baf-c290-42fb-b3c4-877a2d06112f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
