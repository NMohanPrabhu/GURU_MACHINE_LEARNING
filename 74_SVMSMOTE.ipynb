{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed191659-a603-4ee8-9fc7-43c15e26081f",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"6\">Techniques for handling imbalanced datasets</font>\n",
    "<P><font color=\"yELLOW\" size=\"5\"><B>4_SVMSMOTE (SVM Synthetic Minority Over-sampling Technique)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924e651-d03e-4702-b169-1dfa40617638",
   "metadata": {},
   "source": [
    "SVMSMOTE is a variation of SMOTE that combines the Support Vector Machine (SVM) algorithm with the SMOTE technique to generate synthetic samples for the minority class. The idea is to use the SVM classifier to identify the support vectors (the most informative points near the decision boundary) and create synthetic samples around these points.\n",
    "\n",
    "Unlike standard SMOTE, which generates synthetic samples from random minority class samples, SVMSMOTE focuses on generating synthetic samples near the decision boundary and support vectors. This can help improve model performance by creating more meaningful synthetic samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163f08d-c417-42f3-9239-c49b58a7cc08",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>How SVMSMOTE Works:</font>\n",
    "<ol>\n",
    "     <li><font color=\"orange\">Support Vectors Identification</font>\n",
    "<ol>\n",
    "       <li>Support Vectors are the data points that lie closest to the decision boundary between classes and have the greatest impact on the decision boundary of the classifier.</li> \n",
    "        <li>In SVMSMOTE, SVM is used to identify these support vectors.</li></ol></li> \n",
    "       <li><font color=\"orange\">Synthetic Sample Generation:</font>\n",
    "         Once the support vectors are identified, synthetic samples are generated along the line connecting the support vectors of the minority class and other points, rather than randomly in the feature space.</li> \n",
    "      <li><font color=\"orange\"> Focus on Hard-to-Classify Points:</font>\n",
    "         Since support vectors are hard-to-classify points, SVMSMOTE focuses on generating synthetic samples that lie near these critical points, making it more likely to improve the classifier's ability to generalize.</li></ol>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ceca559-1ca0-4a33-bceb-ff4699f29ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SVMSMOTE: Counter({0: 898, 1: 102})\n",
      "Class distribution after SVMSMOTE: Counter({0: 898, 1: 898})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       265\n",
      "           1       0.97      0.97      0.97       274\n",
      "\n",
      "    accuracy                           0.97       539\n",
      "   macro avg       0.97      0.97      0.97       539\n",
      "weighted avg       0.97      0.97      0.97       539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Create an imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, \n",
    "                            n_redundant=10, n_classes=2, weights=[0.9, 0.1], \n",
    "                            random_state=42)\n",
    "\n",
    "# Step 2: Check the class distribution before applying SVMSMOTE\n",
    "print(\"Class distribution before SVMSMOTE:\", Counter(y))\n",
    "\n",
    "# Step 3: Apply SVMSMOTE to oversample the minority class\n",
    "svm_smote = SVMSMOTE(random_state=42)\n",
    "X_resampled, y_resampled = svm_smote.fit_resample(X, y)\n",
    "\n",
    "# Step 4: Check the class distribution after applying SVMSMOTE\n",
    "print(\"Class distribution after SVMSMOTE:\", Counter(y_resampled))\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a classifier (RandomForest) on the resampled data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ba067-02fc-4afb-a6da-8d592af65db8",
   "metadata": {},
   "source": [
    "<font color=\"pink\" size=4>Advantages of SVMSMOTE:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Focuses on Support Vectors:</font> By generating synthetic samples near the support vectors, SVMSMOTE targets the most important samples that are closest to the decision boundary.</li>\n",
    "    <li><font color=\"orange\">Improved Classifier Performance:</font> By generating more meaningful synthetic samples, SVMSMOTE can help improve the classifier's generalization ability.</li>\n",
    "    <li><font color=\"orange\">Less Noise:</font> Compared to SMOTE, which randomly generates synthetic samples across the entire minority class, SVMSMOTE focuses on the most informative samples, potentially reducing noise.</li></ol>\n",
    "\n",
    "<font color=\"pink\" size=4>Drawbacks of SVMSMOTE:</font>\n",
    "<ol>\n",
    "    <li><font color=\"orange\">Computational Complexity:</font> SVMSMOTE is computationally expensive because it requires training an SVM classifier to identify the support vectors before generating synthetic samples.</li>\n",
    "    <li><font color=\"orange\">Risk of Overfitting:</font> By focusing too heavily on the support vectors, there's a risk of overfitting the model to the minority class.</li>\n",
    "    <li><font color=\"orange\">Requires SVM:</font> This method requires the use of an SVM to find the support vectors, which might not be ideal in all scenarios, especially with large datasets.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c5e064-164d-4fd6-b667-7910d5f60d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
